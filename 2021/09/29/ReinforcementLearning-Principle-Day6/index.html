<!DOCTYPE html>
<html lang="en,zh-CN,default">
<head>
  <meta name="google-site-verification" content="3I3e6Cpvx0EA_nboqO9oHdDC0B0kNVp4Ga4lOzKBpnA" />
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zzz0906.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&amp;apos;$&amp;apos;,&amp;apos;$&amp;apos;],[&amp;apos;\\(&amp;apos;,&amp;apos;\\)&amp;apos;]]} });     PrefaceWe start our coursera Sample-based Learning Methods from now on. And in this period, I will still excerpt some sen">
<meta name="keywords" content="ReinforcementLearning">
<meta property="og:type" content="article">
<meta property="og:title" content="ReinforcementLearning-Principle-Day6">
<meta property="og:url" content="https:&#x2F;&#x2F;zzz0906.github.io&#x2F;2021&#x2F;09&#x2F;29&#x2F;ReinforcementLearning-Principle-Day6&#x2F;index.html">
<meta property="og:site_name" content="Zhongzhu&#39;s Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&amp;apos;$&amp;apos;,&amp;apos;$&amp;apos;],[&amp;apos;\\(&amp;apos;,&amp;apos;\\)&amp;apos;]]} });     PrefaceWe start our coursera Sample-based Learning Methods from now on. And in this period, I will still excerpt some sen">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;zzz0906.github.io&#x2F;2021&#x2F;09&#x2F;29&#x2F;ReinforcementLearning-Principle-Day6&#x2F;PiProb.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zzz0906.github.io&#x2F;2021&#x2F;09&#x2F;29&#x2F;ReinforcementLearning-Principle-Day6&#x2F;imSampRa.png">
<meta property="og:updated_time" content="2021-10-08T12:44:29.804Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;zzz0906.github.io&#x2F;2021&#x2F;09&#x2F;29&#x2F;ReinforcementLearning-Principle-Day6&#x2F;PiProb.png">

<link rel="canonical" href="https://zzz0906.github.io/2021/09/29/ReinforcementLearning-Principle-Day6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>ReinforcementLearning-Principle-Day6 | Zhongzhu's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhongzhu's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Keep</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/zzz0906" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zzz0906.github.io/2021/09/29/ReinforcementLearning-Principle-Day6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhongzhu Zhou">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhongzhu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ReinforcementLearning-Principle-Day6
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-29 19:38:36" itemprop="dateCreated datePublished" datetime="2021-09-29T19:38:36+08:00">2021-09-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-08 20:44:29" itemprop="dateModified" datetime="2021-10-08T20:44:29+08:00">2021-10-08</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'],['\\(','\\)']]} }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>We start our coursera Sample-based Learning Methods from now on. And in this period, I will still excerpt some sentences from Sutton’s book. But this time, I will label my own comprehension red.</p>
<a id="more"></a>

<h1 id="ReinforcementLearning-Principle-Day6-Monte-Carlo"><a href="#ReinforcementLearning-Principle-Day6-Monte-Carlo" class="headerlink" title="ReinforcementLearning-Principle-Day6: Monte-Carlo"></a>ReinforcementLearning-Principle-Day6: Monte-Carlo</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns. The term “Monte Carlo” is often used more broadly for any estimation method whose operation involves a significant random component.</p>
<p>Monte Carlo methods sample and average returns for each state–action pair much like the bandit methods we explored in Chapter 2 sample and average rewards for each action. The main di↵erence is that now there are multiple states, each acting like a different bandit problem (like an associative-search or contextual bandit) and the different bandit problems are interrelated. That is, the return after taking an action in one state depends on the actions taken in later states in the same episode. Because all the action selections are undergoing learning, the problem becomes nonstationary from the point of view of the earlier state.</p>
<p>Each of these ideas taken from DP is extended to the Monte Carlo case in which only sample experience is available.</p>
<!-- more -->

<h2 id="5-1-Monte-Carlo-Prediction"><a href="#5-1-Monte-Carlo-Prediction" class="headerlink" title="5.1 Monte Carlo Prediction"></a>5.1 Monte Carlo Prediction</h2><p>The first-visit MC method estimates $v_{\pi}(s)$ as the average of the returns following first visits to s, whereas the every-visit MC method averages the returns following all visits to s. </p>
<p>What is his probability of terminating with a reward of +1 as a function of the dealer’s showing card? All of the probabilities must be computed before DP can be applied, and such computations are often complex and error-prone.</p>
<h2 id="5-2-Monte-Carlo-Estimation-of-Action-Values"><a href="#5-2-Monte-Carlo-Estimation-of-Action-Values" class="headerlink" title="5.2 Monte Carlo Estimation of Action Values"></a>5.2 Monte Carlo Estimation of Action Values</h2><p>If a model is not available, then it is particularly useful to estimate action values (the values of state–action pairs). And the process is the same GPI. generalized policy iteration.</p>
<h2 id="5-3-Monte-Carlo-Control"><a href="#5-3-Monte-Carlo-Control" class="headerlink" title="5.3 Monte Carlo Control"></a>5.3 Monte Carlo Control</h2><p>We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for the Monte Carlo method. One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes.</p>
<p>The pseudocode for Monte Carlo ES is ineffcient because, for each state– action pair, it maintains a list of all returns and repeatedly calculates their mean. It would be more e cient to use techniques similar to those explained in Section 2.4 to maintain just the mean and a count (for each state–action pair) and update them incrementally.</p>
<p>Convergence to this optimal fixed point seems inevitable as the changes to the action-value function decrease over time, <strong>but has not yet been formally proved</strong></p>
<h2 id="5-4-Monte-Carlo-Control-without-Exploring-Starts"><a href="#5-4-Monte-Carlo-Control-without-Exploring-Starts" class="headerlink" title="5.4 Monte Carlo Control without Exploring Starts"></a>5.4 Monte Carlo Control without Exploring Starts</h2><p>here are two approaches to ensuring this, resulting in what we call on-policy methods and on-policy methods. </p>
<ul>
<li>On-policy methods attempt to evaluate or improve the policy that is used to make decisions</li>
<li>whereas off-policy methods evaluate or improve a policy di↵erent from that used to generate the data.</li>
</ul>
<p>Off-policy means we use a policy to generate data and finaly evaluate policy in the end.</p>
<p>In on-policy control methods the policy is generally soft, meaning that $\pi(a|s) &gt; 0$ for all $s \in S$ and all $a \in A(s)$, but gradually shifted closer and closer to a deterministic optimal policy.</p>
<p>The $\epsilon-greedy$ policies are examples of $\epsilon-soft$ policies</p>
<p><span style="color:red">In my opinion, we use montecarlo average first MC control, and for the policy, we will use 1 - epsilon + epsilon / |A| for the maximum return (computed by MC)</span>.</p>
<p>$\pi$ is assured by the policy improvement theorem. Let $\pi’$ be the $\epsilon$-greedy policy.</p>
<h2 id="5-5-Off-policy-Prediction-via-Importance-Sampling"><a href="#5-5-Off-policy-Prediction-via-Importance-Sampling" class="headerlink" title="5.5 Off-policy Prediction via Importance Sampling"></a>5.5 Off-policy Prediction via Importance Sampling</h2><p>one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy being learned about is called the target policy, and the policy used to generate behavior is called the behavior policy.</p>
<p>That is, we require that $\pi(a|s)$ &gt; 0 implies $b(a|s)$ &gt; 0. This is called the assumption of coverage. </p>
<p>Almost all o↵-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another</p>
<p>We apply importance sampling to o↵-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies</p>
<p>called the importance-sampling ratio. Given a starting state St, the probability of the subsequent state–action trajectory, $A_t, S_{t+1}, A_{t+1}, . . . , S_T$ , occurring under any policy $\pi$ is</p>
<p><img src="/2021/09/29/ReinforcementLearning-Principle-Day6/PiProb.png" alt></p>
<p>where p here is the state-transition probability function defined by (3.4). Thus, the relative probability of the trajectory under the target and behavior policies (the importance- sampling ratio) is</p>
<p><img src="/2021/09/29/ReinforcementLearning-Principle-Day6/imSampRa.png" alt></p>
<p>This is where importance sampling comes in. The ratio ⇢t:T  1 transforms the returns to have the right expected value:</p>
<p>$E[p_{t:T-1}G_t | S_t = s] = v_\pi(s)$</p>
<p>When importance sampling is done as a simple average in this way it is called ordinary importance sampling.</p>
<p>Formally, the difference between the first-visit methods of the two kinds of importance sampling is expressed in their biases and variances. Ordinary importance sampling is unbiased whereas weighted importance sampling is biased (though the bias converges asymptotically to zero). On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite (Precup, Sutton, and Dasgupta 2001).</p>
<p>A/A’s variance -&gt; 0 when A-&gt; $\infty$</p>
<p>In practice, the weighted estimator usually has dramatically lower variance and is strongly preferred. Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximation that we explore in the second part of this book.</p>
<p><span style="color:red"> the example 5.5 is intetresting. It use a small example and mathematics formula to tell us why ordinary importance sampling is infinite </span></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ReinforcementLearning/" rel="tag"># ReinforcementLearning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/07/21/ReinforcementLearning-Principle-Day5/" rel="prev" title="ReinforcementLearning-Principle-Day5">
      <i class="fa fa-chevron-left"></i> ReinforcementLearning-Principle-Day5
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/09/29/Operating-System-Memory-Address/" rel="next" title="Operating System Memory Address">
      Operating System Memory Address <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Preface"><span class="nav-number">1.</span> <span class="nav-text">Preface</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ReinforcementLearning-Principle-Day6-Monte-Carlo"><span class="nav-number">2.</span> <span class="nav-text">ReinforcementLearning-Principle-Day6: Monte-Carlo</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">2.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-Monte-Carlo-Prediction"><span class="nav-number">2.2.</span> <span class="nav-text">5.1 Monte Carlo Prediction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-Monte-Carlo-Estimation-of-Action-Values"><span class="nav-number">2.3.</span> <span class="nav-text">5.2 Monte Carlo Estimation of Action Values</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-Monte-Carlo-Control"><span class="nav-number">2.4.</span> <span class="nav-text">5.3 Monte Carlo Control</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-Monte-Carlo-Control-without-Exploring-Starts"><span class="nav-number">2.5.</span> <span class="nav-text">5.4 Monte Carlo Control without Exploring Starts</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-Off-policy-Prediction-via-Importance-Sampling"><span class="nav-number">2.6.</span> <span class="nav-text">5.5 Off-policy Prediction via Importance Sampling</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhongzhu Zhou"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Zhongzhu Zhou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zzz0906" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zzz0906" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/mailto:zhouzhzh8@mail2.sysu.edu.cn" title="E-Mail → mailto:zhouzhzh8@mail2.sysu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com.hk/citations?user=BoZKZl4AAAAJ&hl=zh-CN" title="Google → https:&#x2F;&#x2F;scholar.google.com.hk&#x2F;citations?user=BoZKZl4AAAAJ&amp;hl=zh-CN" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Google</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhongzhu Zhou</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  

  

</body>
</html>
