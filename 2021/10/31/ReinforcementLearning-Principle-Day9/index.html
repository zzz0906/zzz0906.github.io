<!DOCTYPE html>
<html lang="en,zh-CN,default">
<head>
  <meta name="google-site-verification" content="3I3e6Cpvx0EA_nboqO9oHdDC0B0kNVp4Ga4lOzKBpnA" />
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zzz0906.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;],[&#39;\\(&#39;,&#39;\\)&#39;]]} });      Sarsa: On-policy TD Control Q-learning: Off-policy TD Control Maximization Bias and Double Learning Games, Afterstates,">
<meta property="og:type" content="article">
<meta property="og:title" content="ReinforcementLearning-Principle-Day9">
<meta property="og:url" content="https://zzz0906.github.io/2021/10/31/ReinforcementLearning-Principle-Day9/index.html">
<meta property="og:site_name" content="Zhongzhu&#39;s Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;],[&#39;\\(&#39;,&#39;\\)&#39;]]} });      Sarsa: On-policy TD Control Q-learning: Off-policy TD Control Maximization Bias and Double Learning Games, Afterstates,">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zzz0906.github.io/2021/10/31/ReinforcementLearning-Principle-Day9/ESARSA.png">
<meta property="og:image" content="https://zzz0906.github.io/2021/10/31/ReinforcementLearning-Principle-Day9/BDigram.png">
<meta property="og:image" content="https://zzz0906.github.io/2021/10/31/ReinforcementLearning-Principle-Day9/DQ.png">
<meta property="og:image" content="https://zzz0906.github.io/2021/10/31/ReinforcementLearning-Principle-Day9/QScore.png">
<meta property="og:image" content="https://zzz0906.github.io/2021/10/31/ReinforcementLearning-Principle-Day9/IR.png">
<meta property="og:image" content="https://zzz0906.github.io/2021/10/31/ReinforcementLearning-Principle-Day9/ESO.png">
<meta property="article:published_time" content="2021-10-31T08:09:02.000Z">
<meta property="article:modified_time" content="2021-11-07T08:36:53.526Z">
<meta property="article:author" content="Zhongzhu Zhou">
<meta property="article:tag" content="ReinforcementLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zzz0906.github.io/2021/10/31/ReinforcementLearning-Principle-Day9/ESARSA.png">

<link rel="canonical" href="https://zzz0906.github.io/2021/10/31/ReinforcementLearning-Principle-Day9/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>ReinforcementLearning-Principle-Day9 | Zhongzhu's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhongzhu's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Keep</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/zzz0906" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zzz0906.github.io/2021/10/31/ReinforcementLearning-Principle-Day9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhongzhu Zhou">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhongzhu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ReinforcementLearning-Principle-Day9
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-31 16:09:02" itemprop="dateCreated datePublished" datetime="2021-10-31T16:09:02+08:00">2021-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-07 16:36:53" itemprop="dateModified" datetime="2021-11-07T16:36:53+08:00">2021-11-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'],['\\(','\\)']]} }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<ul>
<li>Sarsa: On-policy TD Control</li>
<li>Q-learning: Off-policy TD Control</li>
<li>Maximization Bias and Double Learning</li>
<li>Games, Afterstates, and Other Special Cases</li>
<li>Summary<a id="more"></a></li>
<li>Sarsa: GPI with TD</li>
<li>Sarsa in the Windy Grid World</li>
<li>What is Q-learning</li>
<li>Q-learning in the Windy Grid World</li>
<li>How is Q-learning off-policy</li>
<li>Expected Sarsa</li>
<li>Expected Sarsa in the Cliff World</li>
<li>Generality of Expected Sarsa</li>
<li>Week3 Summary</li>
<li>Program Assignment<h2 id="Sarsa-On-policy-TD-Control"><a href="#Sarsa-On-policy-TD-Control" class="headerlink" title="Sarsa: On-policy TD Control"></a>Sarsa: On-policy TD Control</h2></li>
</ul>
<p>$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha<em>(R_{t+1} + \gamma</em>Q(S_{t+1},A_{t+1}) - Q(S_t,A_t))$</p>
<p>This update is done after every transition from a nonterminal state $S_t$. If $S_{t+1}$ is terminal, then $Q(S_{t+1},A_{t+1})$ is defined as zero.</p>
<h2 id="Q-learning-Off-policy-TD-Control"><a href="#Q-learning-Off-policy-TD-Control" class="headerlink" title="Q-learning: Off-policy TD Control"></a>Q-learning: Off-policy TD Control</h2><p>We can see that, we use original Q and policy to generate next action. But here, we use max Q value as next choice. We still follow policy in the next choince (data generation) in SARSA. Thus SARSA is on-policy. However, when we generate next choice, in Q-learning, instead, we use the best next choice.</p>
<p>This dramatically simplifies the analysis of the algorithm and enabled early convergence proofs.</p>
<h2 id="Expected-Sarsa"><a href="#Expected-Sarsa" class="headerlink" title="Expected Sarsa"></a>Expected Sarsa</h2><p><img src="/2021/10/31/ReinforcementLearning-Principle-Day9/ESARSA.png" alt></p>
<p>Given the next state, $S_{t+1}$, this algorithm moves deterministically in the same direction as Sarsa moves in expectation.</p>
<p>the expected SARSA eliminates the variance due to the SARSA random choice of $A_{t+1}$. Given the same amount of experience we might expect it to perform slightly better than Sarsa, and indeed it generally does.</p>
<p><img src="/2021/10/31/ReinforcementLearning-Principle-Day9/BDigram.png" alt></p>
<p>In such cases, Expected Sarsa can safely set $\alpha$ = 1 without su↵ering any degradation of asymptotic performance, whereas Sarsa can only perform well in the long run at a small value of $\alpha$, at which short-term performance is poor.</p>
<p>Expected Sarsa subsumes and generalizes Q-learning while reliably improving over Sarsa. Except for the small additional computational cost, Expected Sarsa may completely dominate both of the other more-well-known TD control algorithms.</p>
<h2 id="Maximization-Bias-and-Double-Learning"><a href="#Maximization-Bias-and-Double-Learning" class="headerlink" title="Maximization Bias and Double Learning"></a>Maximization Bias and Double Learning</h2><p>a maximum over estimated values is used implicitly as an estimate of the maxim um value, which can lead to a significant positive bias.</p>
<p>The maximum of the true values is zero, but the maximum of the estimates is positive, a positive bias. We call this maximization bias.</p>
<p><span style="color:red"> In my comprehension here, because the maximization agent do not know left choice’s mean is -0.1. it need to explore more, it has more bias to do the exploration? </span></p>
<p>We could then use one estimate, say $Q_1$, to determine the maximizing action $A* = argmaxQ_1(a)$, and the other, $Q_2$, to provide the estimate of its value, $Q_2(A<em>) = Q_2(argmaxa Q_1(a))$. This estimate will then be unbiased in the sense that $E[Q_2(A</em>)] = q(A*)$. We can also repeat the process with the role of the two estimates reversed to yield a second unbiased estimate</p>
<p>If the coin comes up heads, the update is<br><img src="/2021/10/31/ReinforcementLearning-Principle-Day9/DQ.png" alt><br>If the coin comes up tails, then the same update is done with Q1 and Q2 switched, so that $Q_2$ is updated.</p>
<h2 id="Games-Afterstates-and-Other-Special-Cases"><a href="#Games-Afterstates-and-Other-Special-Cases" class="headerlink" title="Games, Afterstates, and Other Special Cases"></a>Games, Afterstates, and Other Special Cases</h2><p>but the state-value function used in tic-tac-toe evaluates board positions after the agent has made its move. Let us call these afterstates, and value functions over these. We know for each possible chess move what the resulting position will be, but not how our opponent will reply. Afterstate value functions are a natural way to take advantage of this kind of knowledge and thereby produce a more e cient learning method. Afterstates arise in many tasks, not just games.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>There is a third way in which TD methods can be extended to control which we did not include in this chapter, called actor–critic methods. These methods are covered in full in Chapter 13.</p>
<p>This is probably due to their great simplicity: they can be applied online, with a minimal amount of computation, to experience generated from interaction with an environment; they can be expressed nearly completely by single equations that can be implemented with small computer programs. </p>
<p>The special cases of TD methods introduced in the present chapter should rightly be called <em>one-step, tabular, model-free</em>TD methods.</p>
<p>TD methods may be relevant to predicting financial data, life spans, election outcomes, weather patterns, animal behavior, demands on power stations, or customer purchases. </p>
<h2 id="Sarsa-GPI-with-TD"><a href="#Sarsa-GPI-with-TD" class="headerlink" title="Sarsa: GPI with TD"></a>Sarsa: GPI with TD</h2><h2 id="Sarsa-in-the-Windy-Grid-World"><a href="#Sarsa-in-the-Windy-Grid-World" class="headerlink" title="Sarsa in the Windy Grid World"></a>Sarsa in the Windy Grid World</h2><p>Windy Gridworld</p>
<p>Notice the episode completion rate stops increasing. This means the agents policy hovers around the optimal policy and won’t be exactly optimal, because of exploration. Notice that Monte Carlo methods would not be a great fit here. This is because many policies do not lead to termination. Monte Carlo methods only learn when an episode terminates. So a deterministic policy might get trapped and never learn a good policy in this gridworld. For example, if the policy took the left action in the start state, it would never terminate.</p>
<h2 id="What-is-Q-learning"><a href="#What-is-Q-learning" class="headerlink" title="What is Q-learning?"></a>What is Q-learning?</h2><p><img src="/2021/10/31/ReinforcementLearning-Principle-Day9/QScore.png" alt></p>
<p>Sarsa is sample-based version of policy iteration which uses Bellman equations for action values, that each depend on a fixed policy. Q-learning is a sample-based version of value iteration which iteratively applies the Bellman optimality equation.</p>
<p><span style="color:red"> So policy iteration vs value iteration? For the policy iteration, the value function is the last policy’s value function (on policy) because we use last policy to generate the sample values. We can use some function to do the earl stop, because maybe currently, greedy is the best choice, we can use best choice to update value function to generate better update (samples) (off policy) and this is value iteration. Q-learning is one of the values iteration. </span></p>
<h2 id="Q-learning-in-the-Windy-Grid-World"><a href="#Q-learning-in-the-Windy-Grid-World" class="headerlink" title="Q-learning in the Windy Grid World"></a>Q-learning in the Windy Grid World</h2><h2 id="How-is-Q-learning-off-policy"><a href="#How-is-Q-learning-off-policy" class="headerlink" title="How is Q-learning off-policy?"></a>How is Q-learning off-policy?</h2><p>If we know the Q-learning update using the $\pi*$, why do we not see any importance sampling ratio?</p>
<p>Because <img src="/2021/10/31/ReinforcementLearning-Principle-Day9/IR.png" alt></p>
<p>We can see only the best choice have the probability zero, thus we do not need the importance sampling.</p>
<h2 id="Expected-Sarsa-1"><a href="#Expected-Sarsa-1" class="headerlink" title="Expected Sarsa"></a>Expected Sarsa</h2><p>cosume more computation resources with lower variance</p>
<h2 id="Expected-Sarsa-in-the-Cliff-World"><a href="#Expected-Sarsa-in-the-Cliff-World" class="headerlink" title="Expected Sarsa in the Cliff World"></a>Expected Sarsa in the Cliff World</h2><p>Sarsa performed better than Q learning on this domain because Sarsa’s policy accounted for its own exploration.</p>
<p>Expected Sarsa’s long-term behavior is unaffected by alpha. Its updates are deterministic in this example. </p>
<p>Therefore the step size only determines how quickly the estimates approach their target values. Sarsa behaves quite differently here, it even fails to converge for larger values of alpha. As alpha decreases, Sarsa’s long run performance approaches expected Sarsa’s.</p>
<h2 id="Generality-of-Expected-SARSA"><a href="#Generality-of-Expected-SARSA" class="headerlink" title="Generality of Expected SARSA"></a>Generality of Expected SARSA</h2><p><img src="/2021/10/31/ReinforcementLearning-Principle-Day9/ESO.png" alt></p>
<p>when pi is greedy, then Q learning == Sarsa.</p>
<p>expectation over actions is computed independently of the action actually selected in the next state. In fact, Pi need not be equal to the behavior policy. This means that Expected Sarsa, like Q-learning, can be used to learn off-policy without importance sampling.</p>
<h2 id="Program-Assignment"><a href="#Program-Assignment" class="headerlink" title="Program Assignment"></a>Program Assignment</h2><p><span style="color:red"> In the assignment, I only learn a matplotlib’s function. The fill between can draw a interval or fluctuation for the line. The results is really great. </span></p>
<h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ReinforcementLearning/" rel="tag"># ReinforcementLearning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/10/20/ReinforcementLearning-Principle-Day8/" rel="prev" title="ReinforcementLearning-Principle-Day8">
      <i class="fa fa-chevron-left"></i> ReinforcementLearning-Principle-Day8
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/11/04/MetaLearning-Standford-Lecture5/" rel="next" title="MetaLearning-Standford-Lecture5">
      MetaLearning-Standford-Lecture5 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sarsa-On-policy-TD-Control"><span class="nav-number">1.</span> <span class="nav-text">Sarsa: On-policy TD Control</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Q-learning-Off-policy-TD-Control"><span class="nav-number">2.</span> <span class="nav-text">Q-learning: Off-policy TD Control</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Expected-Sarsa"><span class="nav-number">3.</span> <span class="nav-text">Expected Sarsa</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Maximization-Bias-and-Double-Learning"><span class="nav-number">4.</span> <span class="nav-text">Maximization Bias and Double Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Games-Afterstates-and-Other-Special-Cases"><span class="nav-number">5.</span> <span class="nav-text">Games, Afterstates, and Other Special Cases</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">6.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sarsa-GPI-with-TD"><span class="nav-number">7.</span> <span class="nav-text">Sarsa: GPI with TD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sarsa-in-the-Windy-Grid-World"><span class="nav-number">8.</span> <span class="nav-text">Sarsa in the Windy Grid World</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-Q-learning"><span class="nav-number">9.</span> <span class="nav-text">What is Q-learning?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Q-learning-in-the-Windy-Grid-World"><span class="nav-number">10.</span> <span class="nav-text">Q-learning in the Windy Grid World</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-is-Q-learning-off-policy"><span class="nav-number">11.</span> <span class="nav-text">How is Q-learning off-policy?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Expected-Sarsa-1"><span class="nav-number">12.</span> <span class="nav-text">Expected Sarsa</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Expected-Sarsa-in-the-Cliff-World"><span class="nav-number">13.</span> <span class="nav-text">Expected Sarsa in the Cliff World</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generality-of-Expected-SARSA"><span class="nav-number">14.</span> <span class="nav-text">Generality of Expected SARSA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Program-Assignment"><span class="nav-number">15.</span> <span class="nav-text">Program Assignment</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary-1"><span class="nav-number">16.</span> <span class="nav-text">Summary</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhongzhu Zhou"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Zhongzhu Zhou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zzz0906" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zzz0906" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhouzhzh8@mail2.sysu.edu.cn" title="E-Mail → mailto:zhouzhzh8@mail2.sysu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com.hk/citations?user=BoZKZl4AAAAJ&hl=zh-CN" title="Google → https:&#x2F;&#x2F;scholar.google.com.hk&#x2F;citations?user=BoZKZl4AAAAJ&amp;hl=zh-CN" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Google</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhongzhu Zhou</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
