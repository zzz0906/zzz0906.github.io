<!DOCTYPE html>
<html lang="en,zh-CN,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zzz0906.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;],[&#39;\\(&#39;,&#39;\\)&#39;]]} });     8.4. Prioritized Sweeping 8.5 Expected vs. Sample Updates 8.6 Trajectory Sampling 8.7 Real-time Dynamic Programming 8.8 P">
<meta property="og:type" content="article">
<meta property="og:title" content="ReinforcementLearning-Principle-Day11">
<meta property="og:url" content="https://zzz0906.github.io/2021/11/14/ReinforcementLearning-Principle-Day11/index.html">
<meta property="og:site_name" content="Zhongzhu&#39;s Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;],[&#39;\\(&#39;,&#39;\\)&#39;]]} });     8.4. Prioritized Sweeping 8.5 Expected vs. Sample Updates 8.6 Trajectory Sampling 8.7 Real-time Dynamic Programming 8.8 P">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zzz0906.github.io/2021/11/14/ReinforcementLearning-Principle-Day11/BDOS.png">
<meta property="og:image" content="https://zzz0906.github.io/2021/11/14/ReinforcementLearning-Principle-Day11/EQ410.png">
<meta property="article:published_time" content="2021-11-14T05:15:34.000Z">
<meta property="article:modified_time" content="2022-05-29T12:16:36.000Z">
<meta property="article:author" content="Zhongzhu &#x2F; Chralie Zhou">
<meta property="article:tag" content="ReinforcementLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zzz0906.github.io/2021/11/14/ReinforcementLearning-Principle-Day11/BDOS.png">

<link rel="canonical" href="https://zzz0906.github.io/2021/11/14/ReinforcementLearning-Principle-Day11/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>ReinforcementLearning-Principle-Day11 | Zhongzhu's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhongzhu's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Keep</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zzz0906.github.io/2021/11/14/ReinforcementLearning-Principle-Day11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhongzhu / Chralie Zhou">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhongzhu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ReinforcementLearning-Principle-Day11
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-14 05:15:34" itemprop="dateCreated datePublished" datetime="2021-11-14T05:15:34+00:00">2021-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-29 12:16:36" itemprop="dateModified" datetime="2022-05-29T12:16:36+00:00">2022-05-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'],['\\(','\\)']]} }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
<ul>
<li>8.4. Prioritized Sweeping</li>
<li>8.5 Expected vs. Sample Updates</li>
<li>8.6 Trajectory Sampling</li>
<li>8.7 Real-time Dynamic Programming</li>
<li>8.8 Planning at Decision Times<a id="more"></a></li>
<li>8.9 Heuristic Search</li>
</ul>
<h2 id="8-4-Prioritized-Sweeping"><a href="#8-4-Prioritized-Sweeping" class="headerlink" title="8.4. Prioritized Sweeping"></a>8.4. Prioritized Sweeping</h2><p>In general, we want to work back not just from goal states but from any state whose value has changed.</p>
<p>In this way one can work backward from arbitrary states that have changed in value, either performing useful updates or terminating the propagation. This general idea might be termed backward focusing of<br>planning computations.</p>
<p>for this algorithm, it add a process:</p>
<p>$P \leftarrow |R+\gamma*max_aQ(S’,a)-Q(S,A)|$</p>
<p>$if P &gt; \theta$ then insert s,a into PQueue</p>
<p>planning the state in the  PQueue and we will update all the S,A lead to the element in the PQueue</p>
<p>It is natural then to update each pair not with a sample update, as we have been using so far</p>
<p>One of prioritized sweeping’s limitations is that it uses expected updates <span style="color:red"> in my view, we call it update expected because we update all the s,a that can leads to current state in the PQueue</span>.</p>
<p>By selecting the order in which small updates are done it is possible to greatly improve planning effciency beyond that possible with prioritized sweeping.</p>
<h2 id="8-5-Expected-vs-Sample-Updates"><a href="#8-5-Expected-vs-Sample-Updates" class="headerlink" title="8.5 Expected vs. Sample Updates"></a>8.5 Expected vs. Sample Updates</h2><ul>
<li>state/state-action</li>
<li>optimal next polciy/arbitary policy</li>
<li>sample/expected</li>
</ul>
<p>These three binary dimensions give rise<br>to eight cases, seven of which corre-<br>spond to specific algorithms, as shown<br>in the figure to the right.</p>
<p><img src="BDOS.png" alt=""></p>
<p>prioritized sweep-ing is always done using one of the expected updates.</p>
<p>expected updates certainly yield a better estimate because they are uncorrupted by sampling error</p>
<p>If there is enough time to complete an expected update, then the resulting estimate is generally better than that of b sample updates because of the absence of sampling error.</p>
<p><span style="color:red"> can we combine expected with sample update (part probability and do one step TD?)</span></p>
<p>By causing estimates to be more accurate sooner, sample updates will have a second advantage in that the values backed up from the successor states will be more accurate. Expected need all successor to be more accurate. But sample is converging during a process.</p>
<h2 id="Trajectory-Sampling"><a href="#Trajectory-Sampling" class="headerlink" title="Trajectory Sampling"></a>Trajectory Sampling</h2><p>Exhaustive sweeps implicitly devote equal time to all parts of the state space rather than focusing where it is needed.</p>
<p>to assure convergence, all states or state–action pairs must be visited in the limit an infinite number of times</p>
<p>If we do not sample uniformly, hore appealing is to distribute updates according to the on-policy distribution, that is, according to the distribution observed when following the current policy.</p>
<p>one simulates explicit individual trajectories and performs updates at the state or state–action pairs encountered along the way. We call this way of generating experience and updates trajectory sampling.</p>
<p>Is the on-policy distribution of updates a good one? Intuitively it seems like a good choice, at least better than the uniform distribution. For example, if you are learning to play chess, you study positions that might arise in real games, not random positions of chess pieces. </p>
<p>Whether or not function approximation is used, one might expect on-policy focusing to significantly improve the speed of planning.</p>
<p><span style="color:red"> Will on-policy update suffer the eplore/epolit problem? like dyna Q+ we give some state we do not reach in long time to do more exploration. If we use on policy, we may acess the same sates in the long time. ans: yes, in the following part, there are experiment to compare sample and on-policy. It’s clear to see the on-policy cannot converge to the optimal solution in the long run.</span></p>
<p>but they do suggest that sampling according to the on-policy distribution can be a great advantage for large problems, in particular for problems in which a small subset of the state–action space is visited under the on-policy distribution.</p>
<h2 id="8-7-Real-time-Dynamic-Programming"><a href="#8-7-Real-time-Dynamic-Programming" class="headerlink" title="8.7 Real-time Dynamic Programming"></a>8.7 Real-time Dynamic Programming</h2><p>RTDP updates the values of states visited in actual or simulated trajectories by means of expected tabular value-iteration updates</p>
<p><img src="EQ410.png" alt=""></p>
<p>We can see the update methods. the RTDP use the mean of each options.</p>
<p>RTDP use a greedy algorithm thus it must have a exploring start.</p>
<h2 id="8-8-Planning-at-Decision-Time"><a href="#8-8-Planning-at-Decision-Time" class="headerlink" title="8.8 Planning at Decision Time"></a>8.8 Planning at Decision Time</h2><p>before an action is selected for any current state $S_t$, planning has played a part in improving the table entries, or the function approximation parameters, needed to select actions for many states, including $S_t$. Used this way, planning is not focused on the current state. We call planning used in this way background planning. <span style="color:red"> I think background planning means we select several state anction pairs to do the update for the value functions like the way we do above </span></p>
<p>More generally, planning used in this way can look much deeper than one-step-ahead and evaluate action choices leading to many different predicted state and reward trajectories. Unlike the first use of planning, here planning focuses on a particular state. We call this decision-time planning. <span style="color:red"> we use planning to predict next state and so on to make the best next choice. </span></p>
<p>These two ways of thinking about planning—using simulated experience to gradually improve a policy or value function, or using simulated experience to select an action for the current state</p>
<h2 id="8-9-Heuristic-Search"><a href="#8-9-Heuristic-Search" class="headerlink" title="8.9 Heuristic Search"></a>8.9 Heuristic Search</h2><p>In conventional heuristic search no e↵ort is made to save the backed-up values by changing the approximate value function. In fact, the value function is generally designed by people and never changed as a result of search.</p>
<p>No matter how you select actions, it is these states and actions that are of highest priority for updates and where you most urgently want your approximate value function to be accurate. =&gt; heuristic search shall focus on these states and actions. </p>
<p>The distribution of updates can be altered in similar ways to focus on the current state and its likely successors.</p>
<h2 id="8-10-Rollout-Algorithms"><a href="#8-10-Rollout-Algorithms" class="headerlink" title="8.10 Rollout Algorithms"></a>8.10 Rollout Algorithms</h2><p>Rollout algorithms are decision-time planning algorithms based on Monte Carlo control applied to simulated trajectories that all begin at the current environment state.</p>
<p>Then the policy that selects an action in s that maximizes these estimates and thereafter follows $\pi$ is a good candidate for a policy that improves over $\pi$</p>
<p>Intuition suggests that the better the rollout policy and the more accurate the value estimates, the better the policy produced by a rollout algorithm is likely be (but see Gelly and Silver, 2007).</p>
<p>We do not ordinarily think of rollout algorithms as learning algorithms because they do not maintain long-term memories of values or policies.</p>
<h2 id="8-11-Monte-Carlo-Tree-Search"><a href="#8-11-Monte-Carlo-Tree-Search" class="headerlink" title="8.11 Monte Carlo Tree Search"></a>8.11 Monte Carlo Tree Search</h2><p>but it is not limited to games; it can be e↵ective for single-agent sequential decision problems if there is an environment model simple enough for fast multistep simulation.</p>
<p>MCTS extends the initial portions of trajectories that have received high evaluations from earlier simulations.</p>
<p>For the most part, the actions in the simulated trajectories are generated using a simple policy, usually called a rollout policy as it is for simpler rollout algorithms.</p>
<p>From the selected node, or from one of its newly-added child nodes (if any), simulation of a complete episode is run with actions selected by the rollout policy. The result is a Monte Carlo trial with actions selected first by the tree policy and beyond the tree by the rollout policy. <span style="color:red"> I think this rollout will do not evaluate the actions and choose actions by rollout policy to generate trajectories and use this rewards gathered by tree policy (need to evaluate multipy nodes and actions) to represent the rewards of actions that are generated by tree policy.</p>
<p>MCTS effectively grows a lookup table to store a partial action-value function, with memory allocated to the estimated values of state–action pairs visited in the initial segments of high-yielding sample trajectories. MCTS thus avoids the problem of globally approximating an action-value function while it retains the benefit of using past experience to guide exploration.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>All of the methods we have explored so far in this book have three key ideas in common: first, they all seek to estimate value functions; second, they all operate by backing up values along actual or possible state trajectories; and third, they all follow the general strategy of generalized policy iteration (GPI), meaning that they maintain an approximate value function and an approximate policy, and they continually try to improve each on the basis of the other.</p>
<p> There are also methods that are intermediate along the horizontal dimension. These include methods that mix expected and sample updates, as well as the possibility of methods that mix samples and distributions within a single update.</p>
<p> all the dimension to represent a method</p>
<ul>
<li>time step</li>
<li>distribution</li>
<li>on vs off polciy</li>
<li>Definition of return</li>
<li>Action values vs. state values vs. afterstate values</li>
<li>Action selection/exploration</li>
<li>Synchronous vs. asynchronous</li>
<li>Real vs. simulated</li>
<li>Location of updates What states or state–action pairs should be updated? Model- free methods can choose only among the states and state–action pairs actually encountered, but model-based methods can choose arbitrarily. There are many possibilities here.</li>
<li>Timing of updates Should updates be done as part of selecting actions, or only after- ward?<ul>
<li>Memory for updates How long should updated values be retained? Should they be retained permanently, or only while computing an action selection, as in heuristic search?</li>
</ul>
</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ReinforcementLearning/" rel="tag"># ReinforcementLearning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/11/07/ReinforcementLearning-Principle-Day10/" rel="prev" title="ReinforcementLearning-Principle-Day10">
      <i class="fa fa-chevron-left"></i> ReinforcementLearning-Principle-Day10
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/11/22/Intel-mac-to-M1-chip-mac/" rel="next" title="Intel mac to M1 chip mac">
      Intel mac to M1 chip mac <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#8-4-Prioritized-Sweeping"><span class="nav-number">1.</span> <span class="nav-text">8.4. Prioritized Sweeping</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-5-Expected-vs-Sample-Updates"><span class="nav-number">2.</span> <span class="nav-text">8.5 Expected vs. Sample Updates</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Trajectory-Sampling"><span class="nav-number">3.</span> <span class="nav-text">Trajectory Sampling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-7-Real-time-Dynamic-Programming"><span class="nav-number">4.</span> <span class="nav-text">8.7 Real-time Dynamic Programming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-8-Planning-at-Decision-Time"><span class="nav-number">5.</span> <span class="nav-text">8.8 Planning at Decision Time</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-9-Heuristic-Search"><span class="nav-number">6.</span> <span class="nav-text">8.9 Heuristic Search</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-10-Rollout-Algorithms"><span class="nav-number">7.</span> <span class="nav-text">8.10 Rollout Algorithms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-11-Monte-Carlo-Tree-Search"><span class="nav-number">8.</span> <span class="nav-text">8.11 Monte Carlo Tree Search</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">9.</span> <span class="nav-text">Summary</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhongzhu / Chralie Zhou"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Zhongzhu / Chralie Zhou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zzz0906" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zzz0906" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhongzhu.zhou@sydney.edu.au" title="E-Mail → mailto:zhongzhu.zhou@sydney.edu.au" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com.hk/citations?user=BoZKZl4AAAAJ&hl=zh-CN" title="Scholar → https:&#x2F;&#x2F;scholar.google.com.hk&#x2F;citations?user&#x3D;BoZKZl4AAAAJ&amp;hl&#x3D;zh-CN" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhongzhu-zhou/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhongzhu-zhou&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin-in fa-fw"></i>Linkedin</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhongzhu / Chralie Zhou</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
