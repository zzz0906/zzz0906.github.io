<!DOCTYPE html>
<html lang="en,zh-CN,default">
<head>
  <meta name="google-site-verification" content="3I3e6Cpvx0EA_nboqO9oHdDC0B0kNVp4Ga4lOzKBpnA" />
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zzz0906.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&amp;apos;$&amp;apos;,&amp;apos;$&amp;apos;],[&amp;apos;\\(&amp;apos;,&amp;apos;\\)&amp;apos;]]} });     Preface 8.1 Models and Planning 8.2 Dyna: Integrated Planning, Acting, and Learning 8.3 When the Model Is Wrong What is a">
<meta name="keywords" content="ReinforcementLearning">
<meta property="og:type" content="article">
<meta property="og:title" content="ReinforcementLearning-Principle-Day10">
<meta property="og:url" content="https:&#x2F;&#x2F;zzz0906.github.io&#x2F;2021&#x2F;11&#x2F;07&#x2F;ReinforcementLearning-Principle-Day10&#x2F;index.html">
<meta property="og:site_name" content="Zhongzhu&#39;s Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&amp;apos;$&amp;apos;,&amp;apos;$&amp;apos;],[&amp;apos;\\(&amp;apos;,&amp;apos;\\)&amp;apos;]]} });     Preface 8.1 Models and Planning 8.2 Dyna: Integrated Planning, Acting, and Learning 8.3 When the Model Is Wrong What is a">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;zzz0906.github.io&#x2F;2021&#x2F;11&#x2F;07&#x2F;ReinforcementLearning-Principle-Day10&#x2F;DynaArch.png">
<meta property="og:image" content="https:&#x2F;&#x2F;zzz0906.github.io&#x2F;2021&#x2F;11&#x2F;07&#x2F;ReinforcementLearning-Principle-Day10&#x2F;Qplan.png">
<meta property="og:updated_time" content="2021-11-14T08:18:40.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;zzz0906.github.io&#x2F;2021&#x2F;11&#x2F;07&#x2F;ReinforcementLearning-Principle-Day10&#x2F;DynaArch.png">

<link rel="canonical" href="https://zzz0906.github.io/2021/11/07/ReinforcementLearning-Principle-Day10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>ReinforcementLearning-Principle-Day10 | Zhongzhu's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhongzhu's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Keep</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/zzz0906" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zzz0906.github.io/2021/11/07/ReinforcementLearning-Principle-Day10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhongzhu Zhou">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhongzhu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ReinforcementLearning-Principle-Day10
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-07 16:38:10" itemprop="dateCreated datePublished" datetime="2021-11-07T16:38:10+11:00">2021-11-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-14 19:18:40" itemprop="dateModified" datetime="2021-11-14T19:18:40+11:00">2021-11-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'],['\\(','\\)']]} }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
<ul>
<li>Preface</li>
<li>8.1 Models and Planning</li>
<li>8.2 Dyna: Integrated Planning, Acting, and Learning</li>
<li>8.3 When the Model Is Wrong</li>
<li>What is a Model? <a id="more"></a></li>
<li>Comparing Sample and Distribution Models</li>
<li>Random Tabular Q-planning</li>
<li>The Dyna Architecture</li>
<li>The Dyna Algorithm</li>
<li>Dyna &amp; Q-learning in a Simple Maze</li>
<li>What if the model is inaccurate?</li>
<li>In-depth with changing environments</li>
<li>Drew Bagnell: self-driving, robotics, and Model Based RL</li>
<li>Week 4 Summary</li>
<li>Programming Assignment: Dyna-Q and Dyna-Q+</li>
</ul>
<h2 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h2><p><span style="color:red">The course skip the n-step Bootstrapping. I think it’s interesting for my time series analysis. I will pick it up after finishing this course.</span></p>
<p>a unified view of reinforcement learning methods that require a model of the environment</p>
<p>These are respectively called model-based and model-free reinforcement learning methods. Model-based methods rely on planning as their primary component, while model-free methods primarily rely on learning.</p>
<p>Our goal in this chapter is a similar integration of model-based and model-free methods. Having established these as distinct in earlier chapters, we now explore the extent to which they can be intermixed.</p>
<h2 id="8-1-Models-and-Planning"><a href="#8-1-Models-and-Planning" class="headerlink" title="8.1 Models and Planning"></a>8.1 Models and Planning</h2><p>By a model of the environment we mean anything that an agent can use to predict how the environment will respond to its actions.</p>
<p>Some models produce a description of all possibilities and their probabilities; these we call distribution models. Other models produce just one of the possibilities, sampled according to the probabilities; these we call sample models.</p>
<p>For example, consider modeling the sum of a dozen dice. A distribution model would produce all possible sums and their probabilities of occurring, whereas a sample model would produce an individual sum drawn according to this probability distribution.</p>
<p>The word planning is used in several di↵erent ways in different fields. We use the term to refer to any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment:</p>
<p>In what we call plan-space planning, planning is instead a search through the space of plans.</p>
<p>Operators transform one plan into another, and value functions, if any, are defined over the space of plans.</p>
<p>Plan-space planning includes evolutionary methods and “partial-order planning,” a common kind of planning in artificial intelligence in which the ordering of steps is not completely determined at all stages of planning. Plan-space methods are diffcult to apply effciently to the stochastic sequential decision problems that are the focus in reinforcement learning, and we do not consider them further (but see, e.g., Russell and Norvig, 2010).</p>
<p>The di↵erence is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment. Of course this di↵erence leads to a number of other di↵erences, for example, in how performance is assessed and in how flexibly experience can be generated. But the common structure means that many ideas and algorithms can be transferred between planning and learning.</p>
<p>This enables planning to be interrupted or redirected at any time with little wasted computation, which appears to be a key requirement for e ciently intermixing planning with acting and with learning of the model. Planning in very small steps may be the most e cient approach even on pure planning problems if the problem is too large to be solved exactly.</p>
<h2 id="8-2-Dyna-Integrated-Planning-Acting-and-Learning"><a href="#8-2-Dyna-Integrated-Planning-Acting-and-Learning" class="headerlink" title="8.2 Dyna: Integrated Planning, Acting, and Learning"></a>8.2 Dyna: Integrated Planning, Acting, and Learning</h2><p>while interacting with the environment, a number of interesting issues arise. New information gained from the interaction may change the model and thereby interact with planning</p>
<p>Dyna-Q includes all of the processes shown in the diagram above—planning, acting, model-learning, and direct RL—all occurring continually.</p>
<p><img src="DynaArch.png" alt=""></p>
<p><img src="Qplan.png" alt=""></p>
<h2 id="8-3-When-the-Model-Is-Wrong"><a href="#8-3-When-the-Model-Is-Wrong" class="headerlink" title="8.3 When the Model Is Wrong"></a>8.3 When the Model Is Wrong</h2><p>In some cases, the suboptimal policy computed by planning quickly leads to the discovery and correction of the modeling error. This tends to happen when the model is optimistic in the sense of predicting greater reward or better state transitions than are actually possible. The planned policy attempts to exploit these opportunities and in doing so discovers that they do not exist.</p>
<p>greater diffculties arise when the environment changes to become better than it was before, and yet the formerly correct policy does not reveal the improvement. In these cases the modeling error may not be detected for a long time, if ever.</p>
<p>Even with an $\epsilon$-greedy policy, it is very unlikely that an agent will take so many exploratory actions as to discover the shortcut.</p>
<p>The Dyna-Q+ agent that did solve the shortcut maze uses one such heuristic. This agent keeps track for each state–action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment. </p>
<p>The more time that has elapsed, the greater (we might presume) the chance that the dynamics of this pair has changed and that the model of it is incorrect.</p>
<p>In particular, if the modeled reward for a transition is r, and the transition has not been tried in T time steps, then planning updates are done as if that transition produced a reward of $r +\sqrt(T)*k$, for some small k. </p>
<p>But there is problem, when shall we use this bonus reward? update or selection or both?</p>
<h2 id="What-is-a-Model"><a href="#What-is-a-Model" class="headerlink" title="What is a Model?"></a>What is a Model?</h2><h2 id="Comparing-Sample-and-Distribution-Models"><a href="#Comparing-Sample-and-Distribution-Models" class="headerlink" title="Comparing Sample and Distribution Models"></a>Comparing Sample and Distribution Models</h2><h2 id="Random-Tabular-Q-planning"><a href="#Random-Tabular-Q-planning" class="headerlink" title="Random Tabular Q-planning"></a>Random Tabular Q-planning</h2><p>This results in some waiting time from after the Learning Update and when the next action is taken. We can fill in this waiting time with Planning Updates. Imagine we have a robot that is standing near a cliff.</p>
<h2 id="The-Dyna-Architecture"><a href="#The-Dyna-Architecture" class="headerlink" title="The Dyna Architecture"></a>The Dyna Architecture</h2><p>In addition, we want to control how the model generates this simulated experience, what states the agent will plan from. We call this process search control.</p>
<h2 id="The-Dyna-Algorithm"><a href="#The-Dyna-Algorithm" class="headerlink" title="The Dyna Algorithm"></a>The Dyna Algorithm</h2><p>search controls selects a previously visited state action pair at random. It must be a state action pair the agent has seen before. Otherwise, the model would not know what happens next. </p>
<h2 id="Dyna-amp-Q-learning-in-a-Simple-Maze"><a href="#Dyna-amp-Q-learning-in-a-Simple-Maze" class="headerlink" title="Dyna &amp; Q-learning in a Simple Maze"></a>Dyna &amp; Q-learning in a Simple Maze</h2><p>The planning update will not have any effect if the sample state action pair produces a 0 T error. This happened a lot in this environment because all the rewards are 0, and so are the initial values. What would happen if we ordered the state action pairs in a more efficient way? It turns out the agent could learn a good policy only using a sixth of the planning updates.</p>
<p>In larger environments random search control becomes even more problematic. If you want to learn more about this topic, check out section 8.4 of the textbook. </p>
<h2 id="What-if-the-model-is-inaccurate"><a href="#What-if-the-model-is-inaccurate" class="headerlink" title="What if the model is inaccurate?"></a>What if the model is inaccurate?</h2><h2 id="In-depth-with-changing-environments"><a href="#In-depth-with-changing-environments" class="headerlink" title="In-depth with changing environments"></a>In-depth with changing environments</h2><p>However, double-checking transitions with low valued actions will often lead to low reward. In changing environments, the agent’s model might become inaccurate at any time. So the agent has to make a choice; explorer to make sure it’s model is accurate, or exploit the model to compute the optimal policy, assuming that the model is correct.</p>
<h2 id="Drew-Bagnell-self-driving-robotics-and-Model-Based-RL"><a href="#Drew-Bagnell-self-driving-robotics-and-Model-Based-RL" class="headerlink" title="Drew Bagnell: self-driving, robotics, and Model Based RL"></a>Drew Bagnell: self-driving, robotics, and Model Based RL</h2><h2 id="Week-4-Summary"><a href="#Week-4-Summary" class="headerlink" title="Week 4 Summary"></a>Week 4 Summary</h2><h2 id="Programming-Assignment-Dyna-Q-and-Dyna-Q"><a href="#Programming-Assignment-Dyna-Q-and-Dyna-Q" class="headerlink" title="Programming Assignment: Dyna-Q and Dyna-Q+"></a>Programming Assignment: Dyna-Q and Dyna-Q+</h2><p>Recall that when adding a state-action to the model, if the agent is visiting the state for the first time, then the remaining actions need to be added to the model as well with zero reward and a transition into itself. (to store the time we do not access them)</p>
<p>When we step once, the tau need to add one except the state action pair we access.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ReinforcementLearning/" rel="tag"># ReinforcementLearning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/11/04/MetaLearning-Standford-Lecture5/" rel="prev" title="MetaLearning-Standford-Lecture5">
      <i class="fa fa-chevron-left"></i> MetaLearning-Standford-Lecture5
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/11/14/ReinforcementLearning-Principle-Day11/" rel="next" title="ReinforcementLearning-Principle-Day11">
      ReinforcementLearning-Principle-Day11 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Preface"><span class="nav-number">1.</span> <span class="nav-text">Preface</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-1-Models-and-Planning"><span class="nav-number">2.</span> <span class="nav-text">8.1 Models and Planning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-2-Dyna-Integrated-Planning-Acting-and-Learning"><span class="nav-number">3.</span> <span class="nav-text">8.2 Dyna: Integrated Planning, Acting, and Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-3-When-the-Model-Is-Wrong"><span class="nav-number">4.</span> <span class="nav-text">8.3 When the Model Is Wrong</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-a-Model"><span class="nav-number">5.</span> <span class="nav-text">What is a Model?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Comparing-Sample-and-Distribution-Models"><span class="nav-number">6.</span> <span class="nav-text">Comparing Sample and Distribution Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Random-Tabular-Q-planning"><span class="nav-number">7.</span> <span class="nav-text">Random Tabular Q-planning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Dyna-Architecture"><span class="nav-number">8.</span> <span class="nav-text">The Dyna Architecture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Dyna-Algorithm"><span class="nav-number">9.</span> <span class="nav-text">The Dyna Algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dyna-amp-Q-learning-in-a-Simple-Maze"><span class="nav-number">10.</span> <span class="nav-text">Dyna &amp; Q-learning in a Simple Maze</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-if-the-model-is-inaccurate"><span class="nav-number">11.</span> <span class="nav-text">What if the model is inaccurate?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#In-depth-with-changing-environments"><span class="nav-number">12.</span> <span class="nav-text">In-depth with changing environments</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Drew-Bagnell-self-driving-robotics-and-Model-Based-RL"><span class="nav-number">13.</span> <span class="nav-text">Drew Bagnell: self-driving, robotics, and Model Based RL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-4-Summary"><span class="nav-number">14.</span> <span class="nav-text">Week 4 Summary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Programming-Assignment-Dyna-Q-and-Dyna-Q"><span class="nav-number">15.</span> <span class="nav-text">Programming Assignment: Dyna-Q and Dyna-Q+</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhongzhu Zhou"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Zhongzhu Zhou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">28</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zzz0906" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zzz0906" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/mailto:zhouzhzh8@mail2.sysu.edu.cn" title="E-Mail → mailto:zhouzhzh8@mail2.sysu.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com.hk/citations?user=BoZKZl4AAAAJ&hl=zh-CN" title="Google → https:&#x2F;&#x2F;scholar.google.com.hk&#x2F;citations?user=BoZKZl4AAAAJ&amp;hl=zh-CN" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Google</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhongzhu Zhou</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  















  

  

  

</body>
</html>
