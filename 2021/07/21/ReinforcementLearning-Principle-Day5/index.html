<!DOCTYPE html>
<html lang="en,zh-CN,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zzz0906.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;],[&#39;\\(&#39;,&#39;\\)&#39;]]} });     Reinforcement Learning Day 4 (Policy Evaluation) Abstract Policy Evaluation Policy Improvement Policy Iteration Value Ite">
<meta property="og:type" content="article">
<meta property="og:title" content="ReinforcementLearning-Principle-Day5">
<meta property="og:url" content="https://zzz0906.github.io/2021/07/21/ReinforcementLearning-Principle-Day5/index.html">
<meta property="og:site_name" content="Zhongzhu&#39;s Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;],[&#39;\\(&#39;,&#39;\\)&#39;]]} });     Reinforcement Learning Day 4 (Policy Evaluation) Abstract Policy Evaluation Policy Improvement Policy Iteration Value Ite">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zzz0906.github.io/2021/07/21/ReinforcementLearning-Principle-Day5/GPI.png">
<meta property="og:image" content="https://zzz0906.github.io/2021/07/21/ReinforcementLearning-Principle-Day5/DP_policy_evaluation.png">
<meta property="article:published_time" content="2021-07-21T07:44:25.000Z">
<meta property="article:modified_time" content="2021-10-08T12:43:59.000Z">
<meta property="article:author" content="Zhongzhu &#x2F; Chralie Zhou">
<meta property="article:tag" content="ReinforcementLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zzz0906.github.io/2021/07/21/ReinforcementLearning-Principle-Day5/GPI.png">

<link rel="canonical" href="https://zzz0906.github.io/2021/07/21/ReinforcementLearning-Principle-Day5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>ReinforcementLearning-Principle-Day5 | Zhongzhu's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhongzhu's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Keep</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zzz0906.github.io/2021/07/21/ReinforcementLearning-Principle-Day5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhongzhu / Chralie Zhou">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhongzhu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ReinforcementLearning-Principle-Day5
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-21 07:44:25" itemprop="dateCreated datePublished" datetime="2021-07-21T07:44:25+00:00">2021-07-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-08 12:43:59" itemprop="dateModified" datetime="2021-10-08T12:43:59+00:00">2021-10-08</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'],['\\(','\\)']]} }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h1 id="Reinforcement-Learning-Day-4-Policy-Evaluation"><a href="#Reinforcement-Learning-Day-4-Policy-Evaluation" class="headerlink" title="Reinforcement Learning Day 4 (Policy Evaluation)"></a>Reinforcement Learning Day 4 (Policy Evaluation)</h1><ul>
<li>Abstract</li>
<li>Policy Evaluation</li>
<li>Policy Improvement</li>
<li>Policy Iteration</li>
<li>Value Iteration<a id="more"></a></li>
<li>Asynchronous Dynamic Programming</li>
<li>Generalized Policy Iteration</li>
<li>Effciency of Dynamic Programming</li>
<li>Summary</li>
<li>Policy Evaluation vs. Control</li>
<li>Iterative Policy Evaluation</li>
<li>Policy Improvement</li>
<li>Policy Iteration</li>
<li>Flexibility of the Policy Iteration Framework</li>
<li>Efficiency of Dynamic Programming</li>
<li>Approximate Dynamic Programming for Fleet Management</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions.</p>
<h2 id="Polciy-Evaluation"><a href="#Polciy-Evaluation" class="headerlink" title="Polciy Evaluation"></a>Polciy Evaluation</h2><p>policy evaluation: compute the state-value function for an arbitrary policy $\pi$</p>
<p><strong>Indeed, the sequence ${v_k}$ can be shown in general to converge to $v_{\pi}$ as $k \rightarrow \infty$ under the same conditions that guarantee the existence of $v_\pi$. This algorithm is called iterative policy evaluation.</strong></p>
<p>successor =&gt; afterwards</p>
<h2 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h2><p>Let see this formula $q_\pi(s,a) = q_\pi(s,\pi’(s)) &gt;= v_\pi(s)$</p>
<p>Then the policy $\pi’$ must be as good as, or better than, $\pi$.</p>
<p>if there is strict inequality of this formula at any state, then there must be strict inequality of $v_\pi’(s) &gt; v_\pi(s)$ at that state.</p>
<p>Thus, It is a natural extension to consider changes at all states, selecting at each state the action that appears best according to $q_\pi(s,a)$. In other words, to consider the new greedy policy, $\pi’$, given by choose the argmax of state action value function.</p>
<p>By construction, the greedy policy meets the conditions of the policy improvement theorem (4.7), so we know that it is as good as, or better than, the original policy. The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of<br>the original policy, is called policy improvement.</p>
<h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><p>Once a policy, $\pi$, has been improved using $\pi$ to yield a better policy, $\pi’$, we can then compute $\pi’$ and improve it again to yield an even better $\pi’’$.</p>
<h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><p>We can update value function directly instead of update vlaue function, update policy.</p>
<p>I consider, it’s a simple idea. But sutton still show use a complex and redudant policy evaluation, improvement, iteration. It’s easy to think, if we can constrcut $v<em>$ to $\pi</em>$, can we update $v$ and let it converge to $v*$?</p>
<h2 id="Asynchronous-Dynamic-Programming"><a href="#Asynchronous-Dynamic-Programming" class="headerlink" title="Asynchronous Dynamic Programming"></a>Asynchronous Dynamic Programming</h2><p>Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized in terms of systematic sweeps of the state set. </p>
<p>Of course, avoiding sweeps does not necessarily mean that we can get away with less computation. It just means that an algorithm does not need to get locked into any hopelessly long sweep before it can make progress improving a policy.</p>
<h2 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h2><p>Almost all reinforcement<br>learning methods are well described as GPI.</p>
<p><img src="GPI.png" alt=""></p>
<p>Making the policy greedy with respect to the value function typically makes the value function incorrect for the changed policy, and making the value function consistent with the policy typically causes that policy no longer to be greedy. In the long run, however, these two processes interact to find a single joint solution: the optimal value function and an<br>optimal policy.</p>
<h2 id="Effciency-of-Dynamic-Programming"><a href="#Effciency-of-Dynamic-Programming" class="headerlink" title="Effciency of Dynamic Programming"></a>Effciency of Dynamic Programming</h2><p>But linear programming methods become impractical at a much smaller number of states than do DP methods (by a factor of about 100). For the largest problems, only DP methods are feasible.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Finally, we note one last special property of DP methods. All of them update estimates of the values of states based on estimates of the values of successor states. That is, they update estimates on the basis of other estimates. We call this general idea <strong>bootstrapping</strong>.</p>
<h2 id="Policy-Evaluation-vs-Control"><a href="#Policy-Evaluation-vs-Control" class="headerlink" title="Policy Evaluation vs. Control"></a>Policy Evaluation vs. Control</h2><p>Control is the task of improving a policy.</p>
<h2 id="Iterative-Policy-Evaluation"><a href="#Iterative-Policy-Evaluation" class="headerlink" title="Iterative Policy Evaluation"></a>Iterative Policy Evaluation</h2><p>We can turn the bellman equations into an update rule, to iteratively compute value functions<br><img src="DP_policy_evaluation.png" alt=""></p>
<h2 id="Policy-Improvement-1"><a href="#Policy-Improvement-1" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h2><p>I have a question: If the $q(s,\pi’(s)) &gt; q(s, \pi(s))$ in some state, but in other states, the $\pi$ is better, but if we update part of action policy from $\pi’$ to $pi$, the following all choices will be affected. Shall we still perform updates? (Does these scenarios still satisfy MDP?)</p>
<p>The summary told me the greedy for $v_\pi$ is better or equal to the original policy. But maybe some actions as described above may affect results?</p>
<p>Because v represent the long term rewards, so it still will be the better policy after upadting?</p>
<h2 id="Polciy-Iteration"><a href="#Polciy-Iteration" class="headerlink" title="Polciy Iteration"></a>Polciy Iteration</h2><p>Combine Policy improvement and iteration</p>
<h2 id="Flexibility-of-the-Policy-Iteration-Framework"><a href="#Flexibility-of-the-Policy-Iteration-Framework" class="headerlink" title="Flexibility of the Policy Iteration Framework"></a>Flexibility of the Policy Iteration Framework</h2><p>Each policy improvement step makes our policy a little more greedy, but not totally greedy. Intuitively, this process should still make progress towards the optimal policy and value function. In fact, the theory tells us the same thing. We will use the term generalized policy iteration to refer to all the ways we can interleave policy evaluation and policy improvement.</p>
<p>The maximum choice for value function in each step and value iteration is one type of avoding full sweeps. We call this Asynchronous dynamic programming.</p>
<h2 id="Efficiency-of-Dynamic-Programming"><a href="#Efficiency-of-Dynamic-Programming" class="headerlink" title="Efficiency of Dynamic Programming"></a>Efficiency of Dynamic Programming</h2><p>Monte Carlo method calculate the average of the state action. And the DP tell me we can use the value we have compute so hard.</p>
<h2 id="Approximate-Dynamic-Programming-for-Fleet-Management"><a href="#Approximate-Dynamic-Programming-for-Fleet-Management" class="headerlink" title="Approximate Dynamic Programming for Fleet Management"></a>Approximate Dynamic Programming for Fleet Management</h2><p>It looks like the prof has two overlap DP problem. And one can sove with LP and the driver’s value can be solved by DP equations. </p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ReinforcementLearning/" rel="tag"># ReinforcementLearning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/04/14/MetaLearning-Standford-Lecture4/" rel="prev" title="MetaLearning-Standford-Lecture4">
      <i class="fa fa-chevron-left"></i> MetaLearning-Standford-Lecture4
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/09/29/ReinforcementLearning-Principle-Day6/" rel="next" title="ReinforcementLearning-Principle-Day6">
      ReinforcementLearning-Principle-Day6 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Reinforcement-Learning-Day-4-Policy-Evaluation"><span class="nav-number">1.</span> <span class="nav-text">Reinforcement Learning Day 4 (Policy Evaluation)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Polciy-Evaluation"><span class="nav-number">1.2.</span> <span class="nav-text">Polciy Evaluation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-Improvement"><span class="nav-number">1.3.</span> <span class="nav-text">Policy Improvement</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-Iteration"><span class="nav-number">1.4.</span> <span class="nav-text">Policy Iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Value-Iteration"><span class="nav-number">1.5.</span> <span class="nav-text">Value Iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Asynchronous-Dynamic-Programming"><span class="nav-number">1.6.</span> <span class="nav-text">Asynchronous Dynamic Programming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generalized-Policy-Iteration"><span class="nav-number">1.7.</span> <span class="nav-text">Generalized Policy Iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Effciency-of-Dynamic-Programming"><span class="nav-number">1.8.</span> <span class="nav-text">Effciency of Dynamic Programming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">1.9.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-Evaluation-vs-Control"><span class="nav-number">1.10.</span> <span class="nav-text">Policy Evaluation vs. Control</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Iterative-Policy-Evaluation"><span class="nav-number">1.11.</span> <span class="nav-text">Iterative Policy Evaluation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-Improvement-1"><span class="nav-number">1.12.</span> <span class="nav-text">Policy Improvement</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Polciy-Iteration"><span class="nav-number">1.13.</span> <span class="nav-text">Polciy Iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Flexibility-of-the-Policy-Iteration-Framework"><span class="nav-number">1.14.</span> <span class="nav-text">Flexibility of the Policy Iteration Framework</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Efficiency-of-Dynamic-Programming"><span class="nav-number">1.15.</span> <span class="nav-text">Efficiency of Dynamic Programming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Approximate-Dynamic-Programming-for-Fleet-Management"><span class="nav-number">1.16.</span> <span class="nav-text">Approximate Dynamic Programming for Fleet Management</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhongzhu / Chralie Zhou"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Zhongzhu / Chralie Zhou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zzz0906" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zzz0906" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhongzhu.zhou@sydney.edu.au" title="E-Mail → mailto:zhongzhu.zhou@sydney.edu.au" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com.hk/citations?user=BoZKZl4AAAAJ&hl=zh-CN" title="Scholar → https:&#x2F;&#x2F;scholar.google.com.hk&#x2F;citations?user&#x3D;BoZKZl4AAAAJ&amp;hl&#x3D;zh-CN" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhongzhu-zhou/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhongzhu-zhou&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin-in fa-fw"></i>Linkedin</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhongzhu / Chralie Zhou</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
