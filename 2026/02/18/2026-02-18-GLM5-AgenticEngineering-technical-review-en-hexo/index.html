<!DOCTYPE html>
<html lang="en,zh-CN,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zzz0906.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="A detailed engineering-focused review of GLM-5 with inline figure&#x2F;table analysis and reproducibility discussion.">
<meta property="og:type" content="article">
<meta property="og:title" content="GLM-5 Technical Review: From Vibe Coding to Agentic Engineering">
<meta property="og:url" content="https://zzz0906.github.io/2026/02/18/2026-02-18-GLM5-AgenticEngineering-technical-review-en-hexo/index.html">
<meta property="og:site_name" content="Zhongzhu&#39;s Blog">
<meta property="og:description" content="A detailed engineering-focused review of GLM-5 with inline figure&#x2F;table analysis and reproducibility discussion.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://arxiv.org/html/2602.15763v1/x5.png">
<meta property="og:image" content="https://arxiv.org/html/2602.15763v1/x4.png">
<meta property="og:image" content="https://arxiv.org/html/2602.15763v1/figures/vending-bench.jpeg">
<meta property="og:image" content="https://arxiv.org/html/2602.15763v1/x1.png">
<meta property="article:published_time" content="2026-02-18T18:05:00.000Z">
<meta property="article:modified_time" content="2026-02-18T18:07:10.760Z">
<meta property="article:author" content="Zhongzhu &#x2F; Chralie Zhou">
<meta property="article:tag" content="GLM-5">
<meta property="article:tag" content="Asynchronous RL">
<meta property="article:tag" content="DSA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://arxiv.org/html/2602.15763v1/x5.png">

<link rel="canonical" href="https://zzz0906.github.io/2026/02/18/2026-02-18-GLM5-AgenticEngineering-technical-review-en-hexo/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>GLM-5 Technical Review: From Vibe Coding to Agentic Engineering | Zhongzhu's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhongzhu's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Keep</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zzz0906.github.io/2026/02/18/2026-02-18-GLM5-AgenticEngineering-technical-review-en-hexo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhongzhu / Chralie Zhou">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhongzhu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          GLM-5 Technical Review: From Vibe Coding to Agentic Engineering
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2026-02-18 18:05:00 / Modified: 18:07:10" itemprop="dateCreated datePublished" datetime="2026-02-18T18:05:00+00:00">2026-02-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Reading/" itemprop="url" rel="index"><span itemprop="name">Paper Reading</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Reading/AI-Agents/" itemprop="url" rel="index"><span itemprop="name">AI Agents</span></a>
                </span>
            </span>

          
            <div class="post-description">A detailed engineering-focused review of GLM-5 with inline figure/table analysis and reproducibility discussion.</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="GLM-5-from-Vibe-Coding-to-Agentic-Engineering-—-Deep-Technical-Review-EN"><a href="#GLM-5-from-Vibe-Coding-to-Agentic-Engineering-—-Deep-Technical-Review-EN" class="headerlink" title="GLM-5: from Vibe Coding to Agentic Engineering — Deep Technical Review (EN)"></a>GLM-5: from Vibe Coding to Agentic Engineering — Deep Technical Review (EN)</h1><p><strong>Author: zhongzhu zhou</strong><br>Paper: GLM-5: from Vibe Coding to Agentic Engineering (arXiv 2602.15763v1, 2026)<br>ArXiv: <a href="https://arxiv.org/abs/2602.15763" target="_blank" rel="noopener">https://arxiv.org/abs/2602.15763</a><br>Project: <a href="https://github.com/zai-org/GLM-5" target="_blank" rel="noopener">https://github.com/zai-org/GLM-5</a></p>
<hr>
<h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h2><ul>
<li>GLM-5 is best read as a full-stack agent-engineering system paper, not only a model-scale update.</li>
<li>The strongest practical story is the combination of DSA long-context efficiency + staged asynchronous RL + realistic agent evaluation.</li>
<li>Evidence is strong on engineering-oriented tasks, with clear progress over prior GLM generations and competitive standing vs proprietary models.</li>
<li>Main caveat remains full external reproducibility at the same scale.</li>
</ul>
<p><strong>Estimated reading time:</strong> ~12–15 minutes</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>I read the GLM-5 paper end-to-end and treated it less as a single-model report and more as a system paper across architecture, data, training infrastructure, post-training RL, and product-facing agent evaluation. My core takeaway is that the paper is trying to solve a concrete transition: from “vibe coding” (short-horizon, prompt-local coding productivity) to “agentic engineering” (long-horizon, stateful, environment-coupled software work).</p>
<p>Technically, the most consequential ingredients are: (1) DSA-based long-context efficiency, (2) asynchronous RL infrastructure and agent RL objectives, (3) explicit long-horizon evaluation (CC-Bench-V2 + SWE-rebench + chain tasks), and (4) aggressive infrastructure work to make the stack practical. The report is unusually broad: 40 pages, strong benchmark spread, and detailed engineering appendices. In this review I explain what GLM-5 does, the prerequisites needed to parse the claims, what I believe is genuinely new, where evidence is strong vs weak, and how one could reproduce parts of it in practice.</p>
<hr>
<h2 id="1-What-this-paper-does"><a href="#1-What-this-paper-does" class="headerlink" title="1) What this paper does"></a>1) What this paper does</h2><p>My reading is straightforward: GLM-5 is a full-stack release aimed at real long-horizon software agents, not a single isolated architecture trick.</p>
<p>The paper makes four high-level claims:</p>
<ol>
<li><strong>Efficiency + scale claim</strong>: Use DSA and recipe changes to scale to a 744B total parameter MoE model (40B active) while preserving long-context quality.</li>
<li><strong>Post-training claim</strong>: Replace/extend synchronous RL pipelines with asynchronous, decoupled rollout/training systems and algorithms that are robust under off-policy drift.</li>
<li><strong>Agentic capability claim</strong>: Improve coding/tool-use/search behavior in realistic environments, not just static leaderboards.</li>
<li><strong>Deployment claim</strong>: Adapt deeply to domestic Chinese chips and optimize kernels/framework paths so the model is operationally practical.</li>
</ol>
<p>I think the paper’s framing is strong because it acknowledges that coding benchmarks alone are insufficient. Their internal CC-Bench-V2 (frontend, backend, long-horizon chains) plus external benchmarks gives a more complete story than “one number on SWE-bench.”</p>
<hr>
<h2 id="2-Prerequisites-I-needed-before-digging-in"><a href="#2-Prerequisites-I-needed-before-digging-in" class="headerlink" title="2) Prerequisites I needed before digging in"></a>2) Prerequisites I needed before digging in</h2><p>Before I explain my judgment, I want to make my own reading assumptions explicit. If these prerequisites are missing, GLM-5 can look like “just a lot of benchmark numbers.” If they are in place, the system story becomes much clearer.</p>
<ul>
<li><strong>MoE economics</strong>: total params vs active params, expert routing overhead, and communication bottlenecks.</li>
<li><strong>Long-context attention families</strong>: dense attention, MLA, SWA, linear variants, and DSA-like sparse retrieval.</li>
<li><strong>Speculative decoding + MTP</strong>: how accept length translates into practical serving throughput.</li>
<li><strong>RL update intuition</strong>: importance ratio, clipping, on-policy/off-policy drift control.</li>
<li><strong>Agent evaluation design</strong>: why pass@1 is insufficient for long-horizon, state-recursive engineering tasks.</li>
</ul>
<h3 id="2-1-What-Muon-means-here-my-engineering-interpretation"><a href="#2-1-What-Muon-means-here-my-engineering-interpretation" class="headerlink" title="2.1 What Muon means here (my engineering interpretation)"></a>2.1 What Muon means here (my engineering interpretation)</h3><p>Let me explain this in a more basic way.</p>
<p>When we train large models, the optimizer is not only “a formula to update weights.” In practice, it is also a major systems component because it decides:</p>
<ul>
<li>how many extra states we must store per parameter,</li>
<li>how much data must be synchronized across devices,</li>
<li>how stable updates remain when training is noisy and long-horizon.</li>
</ul>
<p>In this paper context, Muon is not just a cosmetic optimizer rename. The practical value is that optimizer behavior and distributed execution are co-designed so large-scale training is easier to run stably.</p>
<p>A simple intuition:</p>
<ol>
<li><strong>Every parameter needs update statistics</strong> (e.g., momentum-like terms). Those states consume memory.</li>
<li><strong>In distributed training, these states/gradients must be communicated</strong>. Communication can become the bottleneck.</li>
<li><strong>If updates are too noisy or too aggressive</strong>, training becomes unstable.</li>
</ol>
<p>So Muon-related engineering is trying to keep these three costs in balance: memory, communication, and stability.</p>
<p>From my perspective, this is exactly why the paper’s Muon discussion matters: it is a systems enabler, not only an optimization trick.</p>
<h3 id="2-2-Why-staged-RL-matters-more-than-one-shot-RL"><a href="#2-2-Why-staged-RL-matters-more-than-one-shot-RL" class="headerlink" title="2.2 Why staged RL matters more than one-shot RL"></a>2.2 Why staged RL matters more than one-shot RL</h3><p>A lot of readers casually read “we did RL” as a single phase. I don’t think that interpretation works here. GLM-5’s pipeline is explicitly staged:</p>
<ul>
<li>Reasoning RL first (trajectory quality and answer consistency),</li>
<li>Agentic RL second (tool use + environment interaction),</li>
<li>General RL later (dialog quality, style, emotional alignment).</li>
</ul>
<p>This staging is one reason the report feels internally coherent: each phase targets a different failure mode.</p>
<h3 id="2-3-Sync-RL-vs-async-RL-why-they-moved"><a href="#2-3-Sync-RL-vs-async-RL-why-they-moved" class="headerlink" title="2.3 Sync RL vs async RL (why they moved)"></a>2.3 Sync RL vs async RL (why they moved)</h3><p>In my own words: synchronous RL is simple but wastes compute in long-rollout settings because fast workers wait for slow trajectories. Asynchronous RL decouples rollout and optimization, so utilization is better.</p>
<p>The cost is policy staleness (off-policy drift). That is exactly why importance-ratio control and clipping become operationally central, not theoretical decoration.</p>
<h3 id="2-4-PPO-GRPO-style-intuition-in-plain-language"><a href="#2-4-PPO-GRPO-style-intuition-in-plain-language" class="headerlink" title="2.4 PPO/GRPO-style intuition in plain language"></a>2.4 PPO/GRPO-style intuition in plain language</h3><p>The practical intuition I use is simple:</p>
<ul>
<li>increase probability of high-reward trajectories,</li>
<li>decrease probability of low-reward trajectories,</li>
<li>use ratio/clipping to avoid unstable oversized updates.</li>
</ul>
<p>So the hard part in GLM-5 is not “having an RL formula,” but keeping updates stable under asynchronous, noisy, tool-interactive long trajectories.</p>
<h3 id="2-5-Quick-concept-glossary-basic-but-important"><a href="#2-5-Quick-concept-glossary-basic-but-important" class="headerlink" title="2.5 Quick concept glossary (basic but important)"></a>2.5 Quick concept glossary (basic but important)</h3><ul>
<li><strong>MoE (Mixture of Experts):</strong> only part of the network is activated per token, so total parameters can be huge while per-token compute stays lower.</li>
<li><strong>Active parameters:</strong> the subset of parameters actually used for one forward pass.</li>
<li><strong>DSA (Dynamic Sparse Attention):</strong> an attention strategy that retrieves/selects relevant context instead of attending densely to everything.</li>
<li><strong>MLA:</strong> a memory-efficient long-context attention design used as a strong baseline in this paper.</li>
<li><strong>SWA (Sliding Window Attention):</strong> restricts attention to local windows; cheaper but can lose very long-range dependencies.</li>
<li><strong>MTP (Multi-Token Prediction):</strong> predicts multiple future tokens, often used with speculative decoding to improve throughput.</li>
<li><strong>Accept length:</strong> in speculative decoding, how many drafted tokens are accepted on average; higher usually means better decoding efficiency.</li>
<li><strong>GRPO/PPO-style update:</strong> RL policy update with ratio/clipping mechanisms to reduce instability.</li>
<li><strong>On-policy vs off-policy:</strong> whether training data comes from the current policy or from older policies/logged rollouts.</li>
<li><strong>Policy staleness:</strong> mismatch between data-collection policy and current training policy in async pipelines.</li>
<li><strong>Distillation:</strong> training a target model to imitate a stronger teacher/trajectory distribution for capability recovery and compression.</li>
<li><strong>BSR / ISR / CSR:</strong> build success rate / instance success rate / checklist success rate for agent task evaluation.</li>
<li><strong>Pass@1 limitation:</strong> single-shot correctness metric; often misses long-horizon consistency and interaction failures.</li>
</ul>
<hr>
<h2 id="3-Method-details-—-my-technical-reading"><a href="#3-Method-details-—-my-technical-reading" class="headerlink" title="3) Method details — my technical reading"></a>3) Method details — my technical reading</h2><h2 id="3-1-Pre-training-architecture-choices"><a href="#3-1-Pre-training-architecture-choices" class="headerlink" title="3.1 Pre-training architecture choices"></a>3.1 Pre-training architecture choices</h2><p>From <strong>model size table</strong>, GLM-5 moves from GLM-4.5’s 355B total / 32B active to <strong>744B total / 40B active</strong>, with fewer layers (MoE layers 89→75) and larger hidden dimension. My interpretation:</p>
<ul>
<li>They traded depth for larger expert scale and reduced communication overhead.</li>
<li>This likely helps throughput and engineering stability on large clusters.</li>
<li>The bigger active parameter budget (40B) should help per-token quality, especially on coding/reasoning.</li>
</ul>
<h3 id="MLA-adaptations-and-Muon-Split"><a href="#MLA-adaptations-and-Muon-Split" class="headerlink" title="MLA adaptations and Muon Split"></a>MLA adaptations and Muon Split</h3><p>In <strong>Table 1</strong>, they show baseline comparisons among GQA-8 and MLA variants.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th align="right">Hellaswag</th>
<th align="right">MMLU</th>
<th align="right">C-Eval</th>
<th align="right">RACE</th>
<th align="right">BBH</th>
<th align="right">GSM8K</th>
<th align="right">HumanEval</th>
</tr>
</thead>
<tbody><tr>
<td>GQA-8</td>
<td align="right">77.3</td>
<td align="right">61.2</td>
<td align="right">60.0</td>
<td align="right">79.6</td>
<td align="right">53.3</td>
<td align="right">47.6</td>
<td align="right">38.5</td>
</tr>
<tr>
<td>MLA</td>
<td align="right">77.3</td>
<td align="right">61.5</td>
<td align="right">59.7</td>
<td align="right">77.8</td>
<td align="right">48.9</td>
<td align="right">46.2</td>
<td align="right">33.5</td>
</tr>
<tr>
<td>MLA + Muon Split</td>
<td align="right">77.8</td>
<td align="right">62.5</td>
<td align="right">62.1</td>
<td align="right">79.9</td>
<td align="right">51.8</td>
<td align="right">45.0</td>
<td align="right">36.7</td>
</tr>
<tr>
<td>The key point I took away: plain MLA with their optimizer regime underperforms some GQA metrics, and <strong>Muon Split</strong> appears to recover much of the loss.</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody></table>
<p>This detail matters because many papers present attention substitutions as “plug-and-play.” Here, the authors are explicit that optimizer/parameterization coupling is non-trivial.</p>
<h3 id="MTP-parameter-sharing"><a href="#MTP-parameter-sharing" class="headerlink" title="MTP parameter sharing"></a>MTP parameter sharing</h3><p><strong>Table 2</strong> reports accept length improvement (DeepSeek-V3.2: 2.55 vs GLM-5: 2.76 in their setting).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th align="right">Accept Length</th>
</tr>
</thead>
<tbody><tr>
<td>DeepSeek-V3.2</td>
<td align="right">2.55</td>
</tr>
<tr>
<td>GLM-5</td>
<td align="right">2.76</td>
</tr>
</tbody></table>
<p>As an engineering note, I view this as “small per-token wins that compound at scale.”</p>
<h2 id="3-2-DSA-continuation-strategy"><a href="#3-2-DSA-continuation-strategy" class="headerlink" title="3.2 DSA continuation strategy"></a>3.2 DSA continuation strategy</h2><p>The paper does not merely say “we used sparse attention.” It reports a continuation recipe:</p>
<ul>
<li>Start from a dense/MLA-trained checkpoint.</li>
<li>DSA warmup (indexer adaptation).</li>
<li>Sparse adaptation stage with additional token budget.</li>
</ul>
<p><img src="https://arxiv.org/html/2602.15763v1/x5.png" alt="GLM-5 Figure 6 SFT loss curves"></p>
<p>In <strong>Table 3</strong>, long-context benchmark differences between MLA and DSA are close (and mixed in direction), which supports their claim that DSA can preserve quality while cutting long-context compute.</p>
<table>
<thead>
<tr>
<th></th>
<th align="right">MQ-NIAH-128k</th>
<th align="right">MV-NIAH-128k</th>
<th align="right">SQuAD-128k</th>
<th align="right">HotpotQA-128k</th>
</tr>
</thead>
<tbody><tr>
<td>MLA</td>
<td align="right">100.0</td>
<td align="right">95.5</td>
<td align="right">79.7</td>
<td align="right">66.3</td>
</tr>
<tr>
<td>DSA</td>
<td align="right">100.0</td>
<td align="right">97.0</td>
<td align="right">86.0</td>
<td align="right">63.0</td>
</tr>
</tbody></table>
<p>The ablations in <strong>Table 4/5/6</strong> are among the most valuable parts of the paper for me:</p>
<ul>
<li><strong>Table 4</strong>: naive SWA interleave collapses quickly at longer lengths; search-based pattern helps a lot.</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th align="right">4K</th>
<th align="right">8K</th>
<th align="right">16K</th>
<th align="right">32K</th>
<th align="right">64K</th>
<th align="right">128K</th>
</tr>
</thead>
<tbody><tr>
<td>GLM-9B (Full Attn)</td>
<td align="right">95.19</td>
<td align="right">93.67</td>
<td align="right">92.01</td>
<td align="right">91.09</td>
<td align="right">85.35</td>
<td align="right">75.28</td>
</tr>
<tr>
<td>SWA Interleave</td>
<td align="right">94.87</td>
<td align="right">54.02</td>
<td align="right">25.89</td>
<td align="right">12.61</td>
<td align="right">8.32</td>
<td align="right">6.51</td>
</tr>
<tr>
<td>SWA Pattern</td>
<td align="right">95.78</td>
<td align="right">92.54</td>
<td align="right">88.92</td>
<td align="right">82.52</td>
<td align="right">70.23</td>
<td align="right">53.95</td>
</tr>
<tr>
<td>- <strong>Table 5</strong>: even better efficient variants still show retrieval/task losses at very long lengths.</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr>
<td>- <strong>Table 6</strong>: DSA warmup-only has a long-tail penalty at 128K, but joint training largely closes it.</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody></table>
<p>My read: the paper’s strongest pre-training argument is not “DSA is universally superior,” but “lossless sparse retrieval with proper adaptation can avoid the severe quality cliff seen in naive efficient attention substitutions.”</p>
<h2 id="3-3-Data-and-mid-training-strategy"><a href="#3-3-Data-and-mid-training-strategy" class="headerlink" title="3.3 Data and mid-training strategy"></a>3.3 Data and mid-training strategy</h2><p>The base model budget is reported as <strong>28.5T tokens</strong> overall across stages, with mid-training context progression to <strong>200K</strong>. I liked that they discuss:</p>
<ul>
<li>code corpus expansion and low-resource language classifiers,</li>
<li>long-document filtering and anti-synthetic controls,</li>
<li>repo-level software engineering construction (issue/PR/file context).</li>
</ul>
<p>Even though exact dataset release is unavailable, the conceptual design aligns with their downstream emphasis on long-horizon engineering.</p>
<h2 id="3-4-Training-infrastructure-contributions"><a href="#3-4-Training-infrastructure-contributions" class="headerlink" title="3.4 Training infrastructure contributions"></a>3.4 Training infrastructure contributions</h2><p>This section is underrated. For large models, “algorithmic contribution” without systems support is often non-deployable.</p>
<p>I noted five pragmatic pieces:</p>
<ol>
<li>Flexible MTP placement to reduce stage imbalance.</li>
<li>Pipeline ZeRO2-like gradient sharding and buffer reuse.</li>
<li>Zero-redundant Muon communication paths.</li>
<li>Activation offloading + recomputation overlap.</li>
<li>Sequence-chunked projection/loss to reduce peaks.</li>
</ol>
<p>None of these alone is novel in isolation, but as a combined recipe, they explain how the training plan remains feasible.</p>
<h2 id="3-5-Post-training-RL-stack"><a href="#3-5-Post-training-RL-stack" class="headerlink" title="3.5 Post-training RL stack"></a>3.5 Post-training RL stack</h2><p>The post-training path is staged: SFT → Reasoning RL → Agentic RL → General RL → on-policy cross-stage distillation.</p>
<h3 id="Reasoning-RL"><a href="#Reasoning-RL" class="headerlink" title="Reasoning RL"></a>Reasoning RL</h3><p>They build on GRPO + IcePop-like mismatch handling and explicitly separate training/inference policy distributions. I appreciate that they surface a very practical DSA RL stability issue: non-deterministic top-k behavior can destabilize RL quickly. Their choice to use deterministic <code>torch.topk</code> despite speed cost is a strong “engineering honesty” moment.</p>
<h3 id="Agentic-RL"><a href="#Agentic-RL" class="headerlink" title="Agentic RL"></a>Agentic RL</h3><p>This is central for the paper’s theme. Synchronous RL with long rollouts creates GPU idle time; they decouple rollout and training via orchestrators and add token-level clipping-based off-policy control (double-sided importance sampling style).</p>
<p>In plain words: they are optimizing not just policy quality but end-to-end RL system throughput.</p>
<h3 id="General-RL-human-style-anchoring"><a href="#General-RL-human-style-anchoring" class="headerlink" title="General RL + human style anchoring"></a>General RL + human style anchoring</h3><p>They split objectives into correctness, emotional intelligence, and task-specific quality with hybrid rewards (rules + ORMs + GRMs), and mention explicit use of high-quality human responses as style anchors. I view this as consistent with current best practice for avoiding over-formulaic model tone.</p>
<hr>
<h2 id="4-Experiments-and-benchmark-setup"><a href="#4-Experiments-and-benchmark-setup" class="headerlink" title="4) Experiments and benchmark setup"></a>4) Experiments and benchmark setup</h2><p>Before diving into sub-sections, the paper’s end-to-end training architecture is best read directly from Figure 5, because many benchmark outcomes later are consequences of this staged design (pre-training, long-context mid-training, staged RL, and cross-stage distillation).</p>
<p><img src="https://arxiv.org/html/2602.15763v1/x4.png" alt="GLM-5 Figure 5 training pipeline"></p>
<h2 id="4-1-ARC-benchmark-panel"><a href="#4-1-ARC-benchmark-panel" class="headerlink" title="4.1 ARC benchmark panel"></a>4.1 ARC benchmark panel</h2><p>The main comparison table is <strong>main ARC results table</strong>. I consider it one of the most informative multi-axis summaries in recent model reports because it spans:</p>
<ul>
<li>Reasoning/general (HLE, AIME, HMMT, GPQA, LongBench v2),</li>
<li>Coding (SWE-bench Verified, SWE-bench Multilingual, Terminal-Bench, CyberGym),</li>
<li>Agentic tasks (BrowseComp, τ²-Bench, MCP-Atlas, Tool-Decathlon, Vending Bench 2, GDPval-AA).</li>
</ul>
<p>From main ARC results table, GLM-5’s pattern is:</p>
<ul>
<li>Strong jump over GLM-4.7 in most categories.</li>
<li>Competitive with top proprietary models on many tasks.</li>
<li>Not uniformly best across every benchmark.</li>
</ul>
<p>I appreciate that this pattern is realistic; single-model dominance across all categories is rare.</p>
<h2 id="4-2-Internal-real-world-suite-CC-Bench-V2"><a href="#4-2-Internal-real-world-suite-CC-Bench-V2" class="headerlink" title="4.2 Internal real-world suite: CC-Bench-V2"></a>4.2 Internal real-world suite: CC-Bench-V2</h2><p><strong>CC-Bench results table</strong> + <strong>Figure 10</strong> are critical. The frontend evaluation pipeline introduces “Agent-as-a-Judge”: build verification + interactive GUI exploration with pass/fail checklists. The paper reports high pointwise agreement with human experts and strong ranking correlation.</p>
<p>What I like:</p>
<ul>
<li>It evaluates visible, interactive correctness, not just static outputs.</li>
<li>It includes multiple stacks (HTML/React/Vue/Svelte/Next.js).</li>
<li>It distinguishes BSR/ISR/CSR, so we can see where failures happen.</li>
</ul>
<p>From CC-Bench results table, GLM-5 has high build success and improved CSR, but ISR still trails Claude Opus 4.5 in key stacks. My interpretation: GLM-5 often does many parts correctly but still misses full end-to-end closure.</p>
<h2 id="4-3-Long-horizon-engineering-and-evolving-tasks"><a href="#4-3-Long-horizon-engineering-and-evolving-tasks" class="headerlink" title="4.3 Long-horizon engineering and evolving tasks"></a>4.3 Long-horizon engineering and evolving tasks</h2><p>The long-horizon chain design is compelling: state-recursive multi-step tasks where each step changes repo state and errors compound.</p>
<p><img src="https://arxiv.org/html/2602.15763v1/figures/vending-bench.jpeg" alt="GLM-5 Figure 4 long-horizon tasks"></p>
<p>This addresses a known blind spot of single-commit benchmarks. If a model is good at local edits but poor at maintaining consistency over a chain, normal pass@1 can hide it.</p>
<p>For freshness/generalization, <strong>SWE-rebench table</strong> (SWE-rebench) gives evolving task performance. GLM-5 improves vs GLM-4.7 but remains behind top proprietary entries. I see this as evidence that the paper’s “agentic engineering” goal is directionally successful but not solved.</p>
<hr>
<h2 id="5-Results-interpretation-—-where-I-think-the-evidence-is-strongest"><a href="#5-Results-interpretation-—-where-I-think-the-evidence-is-strongest" class="headerlink" title="5) Results interpretation — where I think the evidence is strongest"></a>5) Results interpretation — where I think the evidence is strongest</h2><h2 id="5-1-Figure-Table-analysis-1-Figure-1-8-benchmark-radar-like-summary"><a href="#5-1-Figure-Table-analysis-1-Figure-1-8-benchmark-radar-like-summary" class="headerlink" title="5.1 Figure/Table analysis #1: Figure 1 (8 benchmark radar-like summary)"></a>5.1 Figure/Table analysis #1: Figure 1 (8 benchmark radar-like summary)</h2><p>In <strong>Figure 1</strong>, GLM-5 is presented against GLM-4.7, DeepSeek-V3.2, Claude Opus 4.5, Gemini 3 Pro, GPT-5.2 across 8 agentic/reasoning/coding tasks.</p>
<p><img src="https://arxiv.org/html/2602.15763v1/x1.png" alt="GLM-5 Figure 1 benchmark overview"></p>
<p>My take:</p>
<ul>
<li>The broad uplift over GLM-4.7 is consistent and not tied to one benchmark family.</li>
<li>GLM-5 appears to close significant portions of the proprietary gap, especially in coding/agentic tasks.</li>
<li>The profile suggests a balanced model rather than a narrow specialist.</li>
</ul>
<h2 id="5-2-Figure-Table-analysis-2-main-ARC-results-table-ARC-main-table"><a href="#5-2-Figure-Table-analysis-2-main-ARC-results-table-ARC-main-table" class="headerlink" title="5.2 Figure/Table analysis #2: main ARC results table (ARC main table)"></a>5.2 Figure/Table analysis #2: main ARC results table (ARC main table)</h2><p><strong>main ARC results table</strong> gives the granular story behind Figure 1. I interpret it as follows:</p>
<ul>
<li>Coding results are among the strongest signals (SWE-bench Verified/Multilingual, Terminal-Bench variants).</li>
<li>Agent benchmarks show strong progress, especially with context management and multi-tool setups.</li>
<li>Reasoning metrics are competitive but heterogeneous, with proprietary models still leading in some scientific/deep-reasoning slots.</li>
</ul>
<p>This distribution matches what we would expect from a pipeline heavily optimized for engineering agents.</p>
<h2 id="5-3-Figure-Table-analysis-3-Table-5-efficient-attention-ablation"><a href="#5-3-Figure-Table-analysis-3-Table-5-efficient-attention-ablation" class="headerlink" title="5.3 Figure/Table analysis #3: Table 5 (efficient attention ablation)"></a>5.3 Figure/Table analysis #3: Table 5 (efficient attention ablation)</h2><p>For me, <strong>Table 5</strong> is a “trust-builder.” Instead of only showing the winning setting, they quantify losses in alternate efficient attention variants across multiple long-context benchmarks.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>RULER</th>
<th>MRCR</th>
<th>HELMET-ICL</th>
<th>RepoQA</th>
</tr>
</thead>
<tbody><tr>
<td>GLM-9B</td>
<td>85.35/75.28</td>
<td>36.53/35.39</td>
<td>77.68/77.36</td>
<td>69.00/65.83</td>
</tr>
<tr>
<td>SWA Interleave</td>
<td>65.94/44.93</td>
<td>30.03/28.83</td>
<td>75.96/63.52</td>
<td>50.33/39.33</td>
</tr>
<tr>
<td>SWA Pattern</td>
<td>83.72/69.59</td>
<td>35.02/33.58</td>
<td>76.48/74.60</td>
<td>62.33/51.17</td>
</tr>
</tbody></table>
<p>I read this as: the team understands the failure modes and is not hiding trade-offs. For practitioners deciding architecture, this ablation is more useful than top-line SOTA claims.</p>
<h2 id="5-4-Figure-Table-analysis-4-CC-Bench-results-table-Figure-10-CC-Bench-V2"><a href="#5-4-Figure-Table-analysis-4-CC-Bench-results-table-Figure-10-CC-Bench-V2" class="headerlink" title="5.4 Figure/Table analysis #4: CC-Bench results table + Figure 10 (CC-Bench-V2)"></a>5.4 Figure/Table analysis #4: CC-Bench results table + Figure 10 (CC-Bench-V2)</h2><p>These are perhaps the most product-relevant artifacts:</p>
<ul>
<li><strong>Figure 10</strong> formalizes a reproducible-ish frontend judging protocol.</li>
<li><strong>CC-Bench results table</strong> separates buildability from true task completion.</li>
</ul>
<p>The ISR gap despite high BSR is especially important. It tells me “code compiles” is no longer enough as a success metric for agentic coding systems.</p>
<h2 id="5-5-Figure-Table-analysis-5-SWE-rebench-table-SWE-rebench"><a href="#5-5-Figure-Table-analysis-5-SWE-rebench-table-SWE-rebench" class="headerlink" title="5.5 Figure/Table analysis #5: SWE-rebench table (SWE-rebench)"></a>5.5 Figure/Table analysis #5: SWE-rebench table (SWE-rebench)</h2><p><strong>SWE-rebench table</strong> gives a more forward-looking check by using newer mined tasks. GLM-5’s gains over GLM-4.7 are present but modest compared with top proprietary peers.</p>
<p>This is exactly the kind of realism I want: progress is substantial, but frontier competition remains tight.</p>
<hr>
<h2 id="6-Limitations-and-boundary-conditions"><a href="#6-Limitations-and-boundary-conditions" class="headerlink" title="6) Limitations and boundary conditions"></a>6) Limitations and boundary conditions</h2><p>I see at least eight limitations/boundaries:</p>
<ol>
<li><strong>Reproducibility depth</strong>: no full data/model release recipe for 28.5T-token pipeline, so exact reproduction is currently impossible for most labs.</li>
<li><strong>Evaluation dependence on internal suites</strong>: CC-Bench-V2 is insightful but not yet a fully community-standard benchmark.</li>
<li><strong>Judge-model risk</strong>: agent-as-judge and model-based evaluation can inherit judge biases despite reported validation.</li>
<li><strong>Async RL complexity</strong>: decoupled systems are operationally harder; small teams may struggle to replicate stability.</li>
<li><strong>DSA implementation sensitivity</strong>: deterministic top-k observations imply kernel/operator details can make or break RL behavior.</li>
<li><strong>Cost opacity</strong>: paper emphasizes efficiency but does not provide complete training FLOPs/cost decomposition.</li>
<li><strong>Long-horizon still unsolved</strong>: chain tasks still show notable gap to top proprietary systems.</li>
<li><strong>Security/safety details limited</strong>: for real agent deployment, safety instrumentation details (policy/runtime guards) are relatively brief.</li>
</ol>
<hr>
<h2 id="7-Reproducibility-and-engineering-notes-practical"><a href="#7-Reproducibility-and-engineering-notes-practical" class="headerlink" title="7) Reproducibility and engineering notes (practical)"></a>7) Reproducibility and engineering notes (practical)</h2><p>If I were trying to reproduce the spirit (not full scale) of this work, I would structure it in layers:</p>
<h3 id="7-1-Minimum-reproducible-stack-research-group-scale"><a href="#7-1-Minimum-reproducible-stack-research-group-scale" class="headerlink" title="7.1 Minimum reproducible stack (research group scale)"></a>7.1 Minimum reproducible stack (research group scale)</h3><ul>
<li>Base model: 7B–32B open MoE/dense model with long-context support.</li>
<li>Attention study: dense vs SWA-pattern vs linear variant vs DSA-like sparse retrieval adaptation.</li>
<li>RL infra: start synchronous first, then decouple rollout/training with explicit telemetry.</li>
<li>Benchmarks: SWE-bench-lite + Terminal-like tasks + one internal long-horizon chain dataset.</li>
</ul>
<h3 id="7-2-Key-instrumentation-I-would-insist-on"><a href="#7-2-Key-instrumentation-I-would-insist-on" class="headerlink" title="7.2 Key instrumentation I would insist on"></a>7.2 Key instrumentation I would insist on</h3><ul>
<li>per-stage GPU utilization during RL,</li>
<li>rollout/training lag and policy staleness histograms,</li>
<li>deterministic/non-deterministic operator flags,</li>
<li>long-context retrieval diagnostics (miss rate vs context position),</li>
<li>chain-task error propagation traces.</li>
</ul>
<h3 id="7-3-Engineering-pitfalls-suggested-by-the-paper"><a href="#7-3-Engineering-pitfalls-suggested-by-the-paper" class="headerlink" title="7.3 Engineering pitfalls suggested by the paper"></a>7.3 Engineering pitfalls suggested by the paper</h3><ul>
<li>Treat top-k/indexer determinism as first-class, not optional.</li>
<li>Do not trust build success as task success.</li>
<li>Expect asynchronous RL to need better observability than classic PPO loops.</li>
<li>Maintain cross-stage distillation or equivalent anti-forgetting mechanism if multi-objective RL is sequential.</li>
</ul>
<hr>
<h2 id="8-How-this-paper-connects-to-the-field"><a href="#8-How-this-paper-connects-to-the-field" class="headerlink" title="8) How this paper connects to the field"></a>8) How this paper connects to the field</h2><p>I place GLM-5 in a trend where model progress is increasingly <strong>systems + data + objective design</strong>, not just larger pre-training runs.</p>
<p>Relative to many model reports, GLM-5 gives unusually concrete evidence that “coding agents” should be evaluated on multi-step stateful execution and GUI/interaction correctness, not only static patch-level tasks.</p>
<p>I also think the DSA + async RL combination is strategic: one reduces long-context compute burden; the other turns saved/system-available capacity into more rollout diversity and faster iteration.</p>
<hr>
<h2 id="9-My-final-assessment"><a href="#9-My-final-assessment" class="headerlink" title="9) My final assessment"></a>9) My final assessment</h2><p>My bottom-line judgment (in my own working style):</p>
<ul>
<li><strong>Strong paper for practitioners building coding/agent systems.</strong></li>
<li><strong>Most valuable contributions are integrated engineering choices</strong>, not a single theorem-level novelty.</li>
<li><strong>Evidence quality is good</strong> across multiple benchmark families, with useful ablations.</li>
<li><strong>Main weakness is full external reproducibility</strong> due to scale/data/infrastructure constraints.</li>
</ul>
<p>If my goal were to improve real-world engineering agents, I would absolutely study this paper carefully. If my goal were strict algorithmic novelty in isolation, I would treat it as a robust systems-integration report with selective algorithmic additions.</p>
<hr>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li>GLM-5 Team. <em>GLM-5: from Vibe Coding to Agentic Engineering</em>. arXiv:2602.15763, 2026.  </li>
<li>Related citations are discussed according to the paper’s own bibliography and benchmark references.</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/GLM-5/" rel="tag"># GLM-5</a>
              <a href="/tags/Agentic-Engineering/" rel="tag"># Agentic Engineering</a>
              <a href="/tags/LLM-Systems/" rel="tag"># LLM Systems</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2026/02/09/react-v5-technical-review-en/" rel="prev" title="ReAct Technical Review: From Reasoning Ability to Executable Reasoning">
      <i class="fa fa-chevron-left"></i> ReAct Technical Review: From Reasoning Ability to Executable Reasoning
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#GLM-5-from-Vibe-Coding-to-Agentic-Engineering-—-Deep-Technical-Review-EN"><span class="nav-number">1.</span> <span class="nav-text">GLM-5: from Vibe Coding to Agentic Engineering — Deep Technical Review (EN)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#TL-DR"><span class="nav-number">1.1.</span> <span class="nav-text">TL;DR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-What-this-paper-does"><span class="nav-number">1.3.</span> <span class="nav-text">1) What this paper does</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Prerequisites-I-needed-before-digging-in"><span class="nav-number">1.4.</span> <span class="nav-text">2) Prerequisites I needed before digging in</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-What-Muon-means-here-my-engineering-interpretation"><span class="nav-number">1.4.1.</span> <span class="nav-text">2.1 What Muon means here (my engineering interpretation)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Why-staged-RL-matters-more-than-one-shot-RL"><span class="nav-number">1.4.2.</span> <span class="nav-text">2.2 Why staged RL matters more than one-shot RL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Sync-RL-vs-async-RL-why-they-moved"><span class="nav-number">1.4.3.</span> <span class="nav-text">2.3 Sync RL vs async RL (why they moved)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-PPO-GRPO-style-intuition-in-plain-language"><span class="nav-number">1.4.4.</span> <span class="nav-text">2.4 PPO&#x2F;GRPO-style intuition in plain language</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-Quick-concept-glossary-basic-but-important"><span class="nav-number">1.4.5.</span> <span class="nav-text">2.5 Quick concept glossary (basic but important)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Method-details-—-my-technical-reading"><span class="nav-number">1.5.</span> <span class="nav-text">3) Method details — my technical reading</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-Pre-training-architecture-choices"><span class="nav-number">1.6.</span> <span class="nav-text">3.1 Pre-training architecture choices</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MLA-adaptations-and-Muon-Split"><span class="nav-number">1.6.1.</span> <span class="nav-text">MLA adaptations and Muon Split</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MTP-parameter-sharing"><span class="nav-number">1.6.2.</span> <span class="nav-text">MTP parameter sharing</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-DSA-continuation-strategy"><span class="nav-number">1.7.</span> <span class="nav-text">3.2 DSA continuation strategy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-Data-and-mid-training-strategy"><span class="nav-number">1.8.</span> <span class="nav-text">3.3 Data and mid-training strategy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-Training-infrastructure-contributions"><span class="nav-number">1.9.</span> <span class="nav-text">3.4 Training infrastructure contributions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-Post-training-RL-stack"><span class="nav-number">1.10.</span> <span class="nav-text">3.5 Post-training RL stack</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reasoning-RL"><span class="nav-number">1.10.1.</span> <span class="nav-text">Reasoning RL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Agentic-RL"><span class="nav-number">1.10.2.</span> <span class="nav-text">Agentic RL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#General-RL-human-style-anchoring"><span class="nav-number">1.10.3.</span> <span class="nav-text">General RL + human style anchoring</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Experiments-and-benchmark-setup"><span class="nav-number">1.11.</span> <span class="nav-text">4) Experiments and benchmark setup</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-ARC-benchmark-panel"><span class="nav-number">1.12.</span> <span class="nav-text">4.1 ARC benchmark panel</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-Internal-real-world-suite-CC-Bench-V2"><span class="nav-number">1.13.</span> <span class="nav-text">4.2 Internal real-world suite: CC-Bench-V2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-Long-horizon-engineering-and-evolving-tasks"><span class="nav-number">1.14.</span> <span class="nav-text">4.3 Long-horizon engineering and evolving tasks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Results-interpretation-—-where-I-think-the-evidence-is-strongest"><span class="nav-number">1.15.</span> <span class="nav-text">5) Results interpretation — where I think the evidence is strongest</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-Figure-Table-analysis-1-Figure-1-8-benchmark-radar-like-summary"><span class="nav-number">1.16.</span> <span class="nav-text">5.1 Figure&#x2F;Table analysis #1: Figure 1 (8 benchmark radar-like summary)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-Figure-Table-analysis-2-main-ARC-results-table-ARC-main-table"><span class="nav-number">1.17.</span> <span class="nav-text">5.2 Figure&#x2F;Table analysis #2: main ARC results table (ARC main table)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-Figure-Table-analysis-3-Table-5-efficient-attention-ablation"><span class="nav-number">1.18.</span> <span class="nav-text">5.3 Figure&#x2F;Table analysis #3: Table 5 (efficient attention ablation)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-Figure-Table-analysis-4-CC-Bench-results-table-Figure-10-CC-Bench-V2"><span class="nav-number">1.19.</span> <span class="nav-text">5.4 Figure&#x2F;Table analysis #4: CC-Bench results table + Figure 10 (CC-Bench-V2)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-Figure-Table-analysis-5-SWE-rebench-table-SWE-rebench"><span class="nav-number">1.20.</span> <span class="nav-text">5.5 Figure&#x2F;Table analysis #5: SWE-rebench table (SWE-rebench)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Limitations-and-boundary-conditions"><span class="nav-number">1.21.</span> <span class="nav-text">6) Limitations and boundary conditions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Reproducibility-and-engineering-notes-practical"><span class="nav-number">1.22.</span> <span class="nav-text">7) Reproducibility and engineering notes (practical)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-Minimum-reproducible-stack-research-group-scale"><span class="nav-number">1.22.1.</span> <span class="nav-text">7.1 Minimum reproducible stack (research group scale)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-Key-instrumentation-I-would-insist-on"><span class="nav-number">1.22.2.</span> <span class="nav-text">7.2 Key instrumentation I would insist on</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-Engineering-pitfalls-suggested-by-the-paper"><span class="nav-number">1.22.3.</span> <span class="nav-text">7.3 Engineering pitfalls suggested by the paper</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-How-this-paper-connects-to-the-field"><span class="nav-number">1.23.</span> <span class="nav-text">8) How this paper connects to the field</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-My-final-assessment"><span class="nav-number">1.24.</span> <span class="nav-text">9) My final assessment</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">1.25.</span> <span class="nav-text">References</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhongzhu / Chralie Zhou"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Zhongzhu / Chralie Zhou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zzz0906" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zzz0906" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhongzhu.zhou@sydney.edu.au" title="E-Mail → mailto:zhongzhu.zhou@sydney.edu.au" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com.hk/citations?user=BoZKZl4AAAAJ&hl=zh-CN" title="Scholar → https:&#x2F;&#x2F;scholar.google.com.hk&#x2F;citations?user&#x3D;BoZKZl4AAAAJ&amp;hl&#x3D;zh-CN" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhongzhu-zhou/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhongzhu-zhou&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin-in fa-fw"></i>Linkedin</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhongzhu / Chralie Zhou</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
