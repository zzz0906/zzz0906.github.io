<!DOCTYPE html>
<html lang="en,zh-CN,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zzz0906.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;],[&#39;\\(&#39;,&#39;\\)&#39;]]} });     Reinforcement Learning Day 3 (Finite Markov Decision Processes) Return, Policy and Value Function Optimal Policies and Op">
<meta property="og:type" content="article">
<meta property="og:title" content="ReinforcementLearning-Principle-Day3">
<meta property="og:url" content="https://zzz0906.github.io/2020/12/02/ReinforcementLearning-Principle-Day3/index.html">
<meta property="og:site_name" content="Zhongzhu&#39;s Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;],[&#39;\\(&#39;,&#39;\\)&#39;]]} });     Reinforcement Learning Day 3 (Finite Markov Decision Processes) Return, Policy and Value Function Optimal Policies and Op">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zzz0906.github.io/2020/12/02/ReinforcementLearning-Principle-Day3/sutton-v-derivation.png">
<meta property="article:published_time" content="2020-12-02T07:30:44.000Z">
<meta property="article:modified_time" content="2021-10-08T12:39:58.000Z">
<meta property="article:author" content="Zhongzhu &#x2F; Chralie Zhou">
<meta property="article:tag" content="ReinforcementLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zzz0906.github.io/2020/12/02/ReinforcementLearning-Principle-Day3/sutton-v-derivation.png">

<link rel="canonical" href="https://zzz0906.github.io/2020/12/02/ReinforcementLearning-Principle-Day3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>ReinforcementLearning-Principle-Day3 | Zhongzhu's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhongzhu's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Keep</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zzz0906.github.io/2020/12/02/ReinforcementLearning-Principle-Day3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhongzhu / Chralie Zhou">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhongzhu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ReinforcementLearning-Principle-Day3
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-12-02 07:30:44" itemprop="dateCreated datePublished" datetime="2020-12-02T07:30:44+00:00">2020-12-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-08 12:39:58" itemprop="dateModified" datetime="2021-10-08T12:39:58+00:00">2021-10-08</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'],['\\(','\\)']]} }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h1 id="Reinforcement-Learning-Day-3-Finite-Markov-Decision-Processes"><a href="#Reinforcement-Learning-Day-3-Finite-Markov-Decision-Processes" class="headerlink" title="Reinforcement Learning Day 3 (Finite Markov Decision Processes)"></a>Reinforcement Learning Day 3 (Finite Markov Decision Processes)</h1><ul>
<li>Return, Policy and Value Function</li>
<li>Optimal Policies and Optimal Value Functions</li>
<li>Coursera False Questions</li>
<li>Optimality and Approximation</li>
<li>Summary<a id="more"></a>

</li>
</ul>
<p><strong>Goal of Reinforcement Learning</strong></p>
<ul>
<li>Micheal Littman: identify where reward signals come from;</li>
<li>develop algorithms that search the space of behaviors to maximize reward signals;</li>
</ul>
<p>MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. </p>
<p>in MDPs we estimate the value q*(s,a) of each action a in each state s, or we estimate the value v*(s) of each state given optimal action selections.</p>
<p>I do not introduce agent-environment here that shall be known before.</p>
<p>In a Markov decision process, the probabilities given by p completely characterize the environment’s dynamics. That is, the probability of each possible value for St and Rt depends on the immediately preceding state and action, $S_{t-1}$ and $A_{t-1}$, and, given them, not at all on earlier states and actions.</p>
<p>In sutton’s book, he will use </p>
<p>$$p(s’,r|s,a) = Pr{S_t = s’,R_t = r| S_{t-1} = s,A_{t-1} = a}$$</p>
<p>This framework may not be suffcient to represent all decision-learning problems usefully, but it has proved to be widely useful and applicable.</p>
<h2 id="Return-Policy-and-Value-Function"><a href="#Return-Policy-and-Value-Function" class="headerlink" title="Return, Policy and Value Function"></a>Return, Policy and Value Function</h2><p>In general, we seek to maximize the expected return, where the return, denoted $G_t = R_{t+1}+..R_T$</p>
<p>where T is a final time step. This approach makes sense in applications in which there is a natural notion of final time step.</p>
<p>Formally, a policy is a mapping from states to probabilities of selecting each possible action.</p>
<p>but goes on continually without limit. For example, this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these continuing tasks. The return formulation (3.7) is problematic for continuing tasks because the final time step would be T = 1, and the return, which is what we are trying to maximize, could easily be infinite</p>
<p>The additional concept that we need is that of discounting. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses At to maximize the expected discounted return:</p>
<p>$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} = \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}$</p>
<p>Almost all reinforcement learning algorithms involve estimating value functions—functions of states (or of state–action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of “how good” here is defined in terms of future rewards that can be expected, or, to be precise, in terms of expected return.</p>
<p><strong>In my opinion, we shall know that the Gt comes from the end of the epoch ($t+1..end$), therefore, we need infer it from the back end.</strong></p>
<p>We can see the below screenshot from sutton’s book of v(s) and v(s’)</p>
<p><img src="./sutton-v-derivation.png" alt=""></p>
<p>Above equation is bellman equation.</p>
<p>Then we can assume a initalize v value for the answer. Then we perform action. And then we get a rewards and use it to conduct the correct v value (include transfer p value and pi value). Then use gradient descent to update the v value!</p>
<p>The Bellman equation (3.14) averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way.</p>
<h2 id="Optimal-Policies-and-Optimal-Value-Functions"><a href="#Optimal-Policies-and-Optimal-Value-Functions" class="headerlink" title="Optimal Policies and Optimal Value Functions"></a>Optimal Policies and Optimal Value Functions</h2><p>In this part, the authors mentioned $v_\pi$ again. Let me review the $v$. The v value only measure the expected return of this state.</p>
<p>if its expected return is greater than or equal to that of ⇡0 for all states. In other words, $\pi &gt; \pi’$ if and only if $v_\pi(s) &gt;= v_{\pi’} (s)$</p>
<p>Comparing to the original policy value function, the optimal policies and optimal value function represent that the maximum over that choice is taken rather than the expected value given some policy.</p>
<p>Once one has $v*$, it is relatively easy to determine an optimal policy. </p>
<h2 id="Coursera-False-Questions"><a href="#Coursera-False-Questions" class="headerlink" title="Coursera False Questions"></a>Coursera False Questions</h2><p>Question 9<br>Case 1: Imagine that you are a vision system. When you are first turned on for the day, an image floods into your camera. You can see lots of things, but not all things. You can’t see objects that are occluded, and of course you can’t see objects that are behind you. After seeing that first scene, do you have access to the Markov state of the environment? </p>
<p>Case 2: Imagine that the vision system never worked properly: it always returned the same static imagine, forever. Would you have access to the Markov state then? (Hint: Reason about $P(S_{t+1} | S_t, …, S_0)$ where $S_t = AllWhitePixels$)</p>
<p>correct answer: You don’t have access to the Markov state in Case 1, but you do have access to the Markov state in Case 2.</p>
<p>Reason: Because there is no history before the first image, the first state has the Markov property. The Markov property does not mean that the state representation tells all that would be useful to know, only that it has not forgotten anything that would be useful to know.</p>
<p>The case when the camera is broken is different, but again we have the Markov property. All the possible futures are the same (all white), so nothing needs to be remembered in order to predict them.</p>
<p>I think the problem want to let me know the markov state has a set or restrict. So the first one we can only use but not is the markov state.</p>
<h2 id="Optimality-and-Approximation"><a href="#Optimality-and-Approximation" class="headerlink" title="Optimality and Approximation"></a>Optimality and Approximation</h2><p>approximations using arrays or tables with one entry for each state (or state–action pair). This we call the tabular case. </p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>I extract several key points from the sutton’s book.</p>
<ol>
<li>states are the basis for making the choices</li>
<li>and the rewards are the basis for evaluating the choices. (basis! not bias!)</li>
<li>the discounted formulation is appropriate for continuing tasks, in which the interaction does not naturally break into episodes but continues without limit.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">We try to define the returns for the two kinds of tasks such that one set of equations can apply to both the episodic and continuing cases.</span><br></pre></td></tr></table></figure>
<p>The two task mentioned above is continous task and discrete task (break into eposides naturlly)</p>
<p><strong>All key points in Sutton’s Book has been presented above.</strong> I think this chapter focus on fundmental modelling idea of main algorithm. (e.g., policy, return ,state-value function). It introduce some mathematics formulas and inference in RL. (e.g., Bellman Equation, Optimal Policy). It help me review equations and inference agian and understand the algorithm better.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ReinforcementLearning/" rel="tag"># ReinforcementLearning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/11/25/Deletes-confusion/" rel="prev" title="HHKB's BS and Delete 按钮引起的疑惑">
      <i class="fa fa-chevron-left"></i> HHKB's BS and Delete 按钮引起的疑惑
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/03/05/ReinforcementLearning-Principle-Day4/" rel="next" title="ReinforcementLearning Principle Day4">
      ReinforcementLearning Principle Day4 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Reinforcement-Learning-Day-3-Finite-Markov-Decision-Processes"><span class="nav-number">1.</span> <span class="nav-text">Reinforcement Learning Day 3 (Finite Markov Decision Processes)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Return-Policy-and-Value-Function"><span class="nav-number">1.1.</span> <span class="nav-text">Return, Policy and Value Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimal-Policies-and-Optimal-Value-Functions"><span class="nav-number">1.2.</span> <span class="nav-text">Optimal Policies and Optimal Value Functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Coursera-False-Questions"><span class="nav-number">1.3.</span> <span class="nav-text">Coursera False Questions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimality-and-Approximation"><span class="nav-number">1.4.</span> <span class="nav-text">Optimality and Approximation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">1.5.</span> <span class="nav-text">Summary</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhongzhu / Chralie Zhou"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Zhongzhu / Chralie Zhou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zzz0906" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zzz0906" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhongzhu.zhou@sydney.edu.au" title="E-Mail → mailto:zhongzhu.zhou@sydney.edu.au" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com.hk/citations?user=BoZKZl4AAAAJ&hl=zh-CN" title="Scholar → https:&#x2F;&#x2F;scholar.google.com.hk&#x2F;citations?user&#x3D;BoZKZl4AAAAJ&amp;hl&#x3D;zh-CN" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhongzhu-zhou/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhongzhu-zhou&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin-in fa-fw"></i>Linkedin</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhongzhu / Chralie Zhou</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
