<!DOCTYPE html>
<html lang="en,zh-CN,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zzz0906.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;],[&#39;\\(&#39;,&#39;\\)&#39;]]} });     Reinforcement Learning Day 2 (Multi-armed Bandits) Q* Formula Nonstationary problem Optimistic Initial Value Gradient Ban">
<meta property="og:type" content="article">
<meta property="og:title" content="ReinforcementLearning-Principle-Day2">
<meta property="og:url" content="https://zzz0906.github.io/2020/10/30/ReinforcementLearning-Principle-Day2/index.html">
<meta property="og:site_name" content="Zhongzhu&#39;s Blog">
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;],[&#39;\\(&#39;,&#39;\\)&#39;]]} });     Reinforcement Learning Day 2 (Multi-armed Bandits) Q* Formula Nonstationary problem Optimistic Initial Value Gradient Ban">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-10-30T06:06:57.000Z">
<meta property="article:modified_time" content="2021-10-08T12:44:07.000Z">
<meta property="article:author" content="Zhongzhu &#x2F; Chralie Zhou">
<meta property="article:tag" content="ReinforcementLearning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zzz0906.github.io/2020/10/30/ReinforcementLearning-Principle-Day2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>ReinforcementLearning-Principle-Day2 | Zhongzhu's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhongzhu's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Keep</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zzz0906.github.io/2020/10/30/ReinforcementLearning-Principle-Day2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Zhongzhu / Chralie Zhou">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhongzhu's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ReinforcementLearning-Principle-Day2
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-10-30 06:06:57" itemprop="dateCreated datePublished" datetime="2020-10-30T06:06:57+00:00">2020-10-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-08 12:44:07" itemprop="dateModified" datetime="2021-10-08T12:44:07+00:00">2021-10-08</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'],['\\(','\\)']]} }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h1 id="Reinforcement-Learning-Day-2-Multi-armed-Bandits"><a href="#Reinforcement-Learning-Day-2-Multi-armed-Bandits" class="headerlink" title="Reinforcement Learning Day 2 (Multi-armed Bandits)"></a>Reinforcement Learning Day 2 (Multi-armed Bandits)</h1><ul>
<li>Q* Formula</li>
<li>Nonstationary problem</li>
<li>Optimistic Initial Value</li>
<li>Gradient Bandits Problem</li>
<li>Associative Search<a id="more"></a></li>
<li>Summary</li>
<li>Overall</li>
</ul>
<p>The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions.</p>
<p>This chapter is implementing RL in a simple setting. I also learn this chapter in the coursera <em>fundamentals-of-reinforcement-learning</em> by Martha and Adam in University of Alberta.</p>
<p>I learn three algorithms or two algorithms fully in coursera. First, it’s the formula to compute Q* and epsilon-greedy algorithm. This algorithm<br><strong>Studying this case enables us to see most clearly how evaluative feedback diers from, and yet can be combined with, instructive feedback.</strong></p>
<p>I found richard keep talking about the difference between RL and other learning algorithms. </p>
<h3 id="Q-Formula"><a href="#Q-Formula" class="headerlink" title="Q* Formula"></a>Q* Formula</h3><p>(Assuming we have understand what is the K-arm bandit’s meaning)</p>
<p>Q* function in K-arm bandit is a problem of </p>
<p>$$Q_*(a) = \mathbb{E}(R_t|A_t=a)$$ </p>
<p>We denote the estimated value of action a at time step t as Qt(a). We would like $Q_{t}(a)$ to be close to $q_*(a)$.</p>
<p>$NewEstimate\leftarrow OldEstimate+Stepsize[Target-OldEstimate]$</p>
<h2 id="Nonstationary-Problem"><a href="#Nonstationary-Problem" class="headerlink" title="Nonstationary Problem"></a>Nonstationary Problem</h2><p>stationary bandit problems, that is, for bandit problems in which the reward probabilities do not change over time.</p>
<p>Therefore, we need to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter.</p>
<p>It turn into our familiar formula:</p>
<p>$$Q_{n+1} = Q_n + \alpha * [R_n - Q_n] $$</p>
<p>We use this time’s difference and add it to our estimated values</p>
<p>And keep deriving it:</p>
<p>$$= \alpha * R_n + (1 - \alpha) * Q_n $$<br>$$= \alpha * R_n + (1 - \alpha) * (\alpha * R_{n-1} + (1-\alpha) * Q_{n-1})$$</p>
<p>reduce it until $Q_1$</p>
<p>$$= (1- \alpha )^n*Q_1 + \sum_{i=1}^n \alpha *(1- \alpha )^{n-i} * R_i$$</p>
<p>If $1 - \alpha = 0$, then all the weight goes on the very last reward, Rn, because of the convention that $0^0= 1$.</p>
<p>If we set $\alpha = \frac{1}{n}$ which means with the growing of our game, the reward given by the higher step shall be less importance? It seems not make sense for this problem? </p>
<p>It means the more far step we shall believe our past experience and convergence. And through the code assignment we know the importance of the $\frac{1}{n}$!</p>
<p><strong>Oh because it’s a non stationary problem!</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Does this mean that 1&#x2F;N(A) is always the best? When might it not be? One possible setting where it might not be as effective is in non-stationary problems. You learned about non-stationarity in the lessons. Non-stationarity means that the environment may change over time. This could manifest itself as continual change over time of the environment, or a sudden change in the environment.</span><br></pre></td></tr></table></figure>

<p>The smaller step size cannot let the estimated value be close to actual value and bigger will fluctuate. Therefore, we need to let step size bigger in the stationary problem.</p>
<p><strong>Conclusion in coursera work</strong></p>
<p>These are the types of tradeoffs we have to think about in reinforcement learning. A larger step size moves us more quickly toward the true value, but can make our estimated values oscillate around the expected value. A step size that reduces over time can converge to close to the expected value, without oscillating. On the other hand, such a decaying stepsize is not able to adapt to changes in the environment. Nonstationarity—and the related concept of partial observability—is a common feature of reinforcement learning problems and when learning online.</p>
<p>Therefore, above content describe the choic of stepsize. And sutton tell us: A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1:<br>$$ \sum_{n=1}^{\infty}\alpha_n(a) = \infty$$<br>$$ \sum_{n=1}^{\infty}\alpha^2_n(a) &lt; \infty$$</p>
<h2 id="Optimistic-Initial-Value"><a href="#Optimistic-Initial-Value" class="headerlink" title="Optimistic Initial Value"></a>Optimistic Initial Value</h2><p>In the language of statistics, these methods are biased by their initial estimates. </p>
<p>We call this technique for encouraging exploration optimistic initial values. We regard it as a simple trick that can be quite e↵ective on stationary problems,</p>
<p>The beginning of time occurs only once, and thus we should not focus on it too much.</p>
<p><strong>BTW: UCB let the Q = Q_value + Uncertainty (which means we can choose one arm if it has a huge expected / huge uncertainty)</strong></p>
<h2 id="Gradient-Bandit-Algorithms"><a href="#Gradient-Bandit-Algorithms" class="headerlink" title="Gradient Bandit Algorithms"></a>Gradient Bandit Algorithms</h2><p>In this section we consider learning a numerical preference for each action a, which we denote $H_t(a) \in R$. </p>
<p>$$Pr{A_t = a} = \frac{e^H_t(a)}{\sum_{b=1}^ke^H_t(b)} = \pi_t(A_t)$$ </p>
<p>$\pi_t(a)$, for the probability of taking action a at time t. (policy)</p>
<p>There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. On each step, after selecting action At and receiving the reward Rt, the action preferences are updated by:</p>
<p>$$H_{t+1}(A_t) = H_t(At) + \alpha<em>(R_t-R_{average})</em>(1-\pi_tA_t) $$ </p>
<p>(infered from gradient decent)</p>
<p>we want to get the real probability, and the real one is R_t and parameter is R_{average} (like the Neural network).</p>
<p>$H_{t+1}(a) = H_t(a) + \alpha *\frac{\delta E[R_t]}{\delta H_t(a)}$</p>
<p>we want to find the minimum $H_t(a)$ to let the expected reward -&gt; maximum or (Rreal-Raverage = 0)’ for $H_t(a)$</p>
<p>$E[R_t] = \sum_x{\pi_t(x)<em>q_</em>(x)}{}$</p>
<p>And we can infer from this formula to the latter formula.</p>
<p>Sutton’s derivation is gorgeous [page 39 - 40]</p>
<p>And the trick is multiply $\pi_t(x)$ and \ $\pi_t(x)$ at the same time. </p>
<p>Baseline * $\pi_t(A_t)$</p>
<p>I understand the policy’s gradient’s algorithm. </p>
<p>We update the value which related to the polciy’s probability directly.</p>
<h2 id="Associative-Search"><a href="#Associative-Search" class="headerlink" title="Associative Search"></a>Associative Search</h2><p>We present this problem in the next chapter and consider its ramifications throughout the rest of the book.</p>
<p>(current reward will affect the next selection)</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>The Gittins-index approach is an instance of Bayesian methods, which assume a known initial distribution over the action values and then update the distribution exactly after each step (assuming that the true action values are stationary). In general, the update computations can be very complex, but for certain special distributions (called conjugate priors) they are easy. One possibility is to then select actions at each step according to their posterior probability of being the best action. This method, sometimes called posterior sampling or Thompson sampling, often performs similarly to the best of the distribution-free methods we have presented in this chapter.</p>
<p>(I see many algorithms use Gittins index as the benchmark) (it’s the assumption best solution?) Let me keep reading this book.</p>
<h3 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h3><p>In this chapter, we learn some simple but important concepts including the Q-value, $\epsilon$-greedy, step-size, stationary and non-stationary, optimistic initial value, gradient based (policy gradient problem) and associative search. I found that after I learn RL, in fact, this chapter describe some simple version of the complex algorithm. And it hint me some point I never know (inital value (MAML)). This chapter is a great beginning of the RL. Let me keep exploring more specific and complex algorithm.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ReinforcementLearning/" rel="tag"># ReinforcementLearning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/09/25/Slurm-Day5/" rel="prev" title="Slurm-Day5">
      <i class="fa fa-chevron-left"></i> Slurm-Day5
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/11/04/MetaLearning-Standford-Lecture2/" rel="next" title="MetaLearning-Standford-Lecture2">
      MetaLearning-Standford-Lecture2 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Reinforcement-Learning-Day-2-Multi-armed-Bandits"><span class="nav-number">1.</span> <span class="nav-text">Reinforcement Learning Day 2 (Multi-armed Bandits)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Q-Formula"><span class="nav-number">1.0.1.</span> <span class="nav-text">Q* Formula</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Nonstationary-Problem"><span class="nav-number">1.1.</span> <span class="nav-text">Nonstationary Problem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimistic-Initial-Value"><span class="nav-number">1.2.</span> <span class="nav-text">Optimistic Initial Value</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Bandit-Algorithms"><span class="nav-number">1.3.</span> <span class="nav-text">Gradient Bandit Algorithms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Associative-Search"><span class="nav-number">1.4.</span> <span class="nav-text">Associative Search</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">1.5.</span> <span class="nav-text">Summary</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Overall"><span class="nav-number">1.5.1.</span> <span class="nav-text">Overall</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhongzhu / Chralie Zhou"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Zhongzhu / Chralie Zhou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zzz0906" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zzz0906" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhongzhu.zhou@sydney.edu.au" title="E-Mail → mailto:zhongzhu.zhou@sydney.edu.au" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com.hk/citations?user=BoZKZl4AAAAJ&hl=zh-CN" title="Scholar → https:&#x2F;&#x2F;scholar.google.com.hk&#x2F;citations?user&#x3D;BoZKZl4AAAAJ&amp;hl&#x3D;zh-CN" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhongzhu-zhou/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhongzhu-zhou&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin-in fa-fw"></i>Linkedin</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhongzhu / Chralie Zhou</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
